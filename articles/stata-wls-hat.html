<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>How Stata's hat matrix differs with weights • estimatr</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="How Stata's hat matrix differs with weights">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">estimatr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.0.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/getting-started.html">Getting started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/getting-started.html">Getting started using estimatr</a></li>
    <li><a class="dropdown-item" href="../articles/absorbing-fixed-effects.html">Absorbing Fixed Effects with estimatr</a></li>
    <li><a class="dropdown-item" href="../articles/estimatr-in-the-tidyverse.html">estimatr in the Tidyverse</a></li>
    <li><a class="dropdown-item" href="../articles/benchmarking-estimatr.html">Benchmarking estimatr</a></li>
    <li><a class="dropdown-item" href="../articles/emmeans-examples.html">Examples with emmeans</a></li>
    <li><a class="dropdown-item" href="../articles/mathematical-notes.html">Mathematical notes for estimatr</a></li>
    <li><a class="dropdown-item" href="../articles/regression-tables.html">Regression Tables with estimatr</a></li>
    <li><a class="dropdown-item" href="../articles/simulations-debiasing-dim.html">Simulations - Debiasing Difference-in-Means</a></li>
    <li><a class="dropdown-item" href="../articles/simulations-ols-variance.html">Simulations - OLS and Variance</a></li>
    <li><a class="dropdown-item" href="../articles/stata-wls-hat.html">How Stata's hat matrix differs with weights</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-software" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Software</button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-software">
<li><a class="external-link dropdown-item" href="http://declaredesign.org/r/declaredesign/">DeclareDesign</a></li>
    <li><a class="external-link dropdown-item" href="https://declaredesign.org/r/randomizr/">randomizr</a></li>
    <li><a class="external-link dropdown-item" href="https://declaredesign.org/r/fabricatr/">fabricatr</a></li>
    <li><a class="dropdown-item" href="https://declaredesign.org/r/estimatr/">estimatr</a></li>
    <li><a class="external-link dropdown-item" href="https://declaredesign.org/r/designlibrary/">DesignLibrary</a></li>
    <li><a class="external-link dropdown-item" href="https://eos.wzb.eu/ipi/DDWizard/">DesignWizard</a></li>
  </ul>
</li>
<li class="nav-item"><a class="external-link nav-link" href="https://declaredesign.org">declaredesign.org</a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<script src="stata-wls-hat_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>How Stata's hat matrix differs with weights</h1>
                        <h4 data-toc-skip class="author">Luke Sonnet</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/DeclareDesign/estimatr/blob/main/vignettes/stata-wls-hat.Rmd" class="external-link"><code>vignettes/stata-wls-hat.Rmd</code></a></small>
      <div class="d-none name"><code>stata-wls-hat.Rmd</code></div>
    </div>

    
    
<p>Researchers use linear regression with heteroskedasticity-robust standard errors. Many social scientists use either Stata or R. One would hope the two would always agree in their estimates. Unfortunately, estimating weighted least squares with HC2 or HC3 robust variance results in different answers across Stata and common approaches in R as well as Python.</p>
<p>The discrepancy is due to differences in how the software estimates the “hat” matrix, on which both HC2 and HC3 variance estimators rely. The short story is that Stata estimates the hat matrix as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top
</annotation></semantics></math></p>
<p>while the usual approaches in R, including <a href="https://CRAN.R-project.org/package=sandwich" class="external-link"><code>sandwich</code></a> and <a href="/r/estimatr/"><code>estimatr</code></a>, and Python (e.g. <a href="http://www.statsmodels.org/stable/index.html" class="external-link"><code>statsmodels</code></a>) estimate the following hat matrix</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle></mrow><annotation encoding="application/x-tex">
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}
</annotation></semantics></math></p>
<p>This results in differences when researches estimate HC2 and HC3 variance estimators. The HC1 standard errors, Stata’s default, are the same across all packages. The rest of this document just walks through the set-up for the above and demonstrates some results from Stata, R, and Python.</p>
<div class="section level3">
<h3 id="weighted-least-squares">Weighted least squares<a class="anchor" aria-label="anchor" href="#weighted-least-squares"></a>
</h3>
<p>Let’s briefly review WLS. Weights are used in linear regression often for two key problems; (1) to model and correct for heteroskedasticity, and (2) to deal with unequal sampling (or treatment) probabilities. In both cases, we take the standard model</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><msubsup><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>i</mi><mi>⊤</mi></msubsup><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo>+</mo><msub><mi>ϵ</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
y_i = \mathbf{x}_i^\top \mathbf{\beta} + \epsilon_i,
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th unit’s outcome, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>i</mi></msub><annotation encoding="application/x-tex">\mathbf{x}_i</annotation></semantics></math> is a column vector of covariates, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><annotation encoding="application/x-tex">\mathbf{\beta}</annotation></semantics></math> are the coefficients of interest, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is some error, and rescale the model by the square root of that unit’s weight, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>w</mi><mi>i</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{w_i}</annotation></semantics></math>. Our model then becomes</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><msub><mi>y</mi><mi>i</mi></msub><msqrt><msub><mi>w</mi><mi>i</mi></msub></msqrt></mfrac><mo>=</mo><mfrac><msubsup><mstyle mathvariant="bold"><mi>𝐱</mi></mstyle><mi>i</mi><mi>⊤</mi></msubsup><msqrt><msub><mi>w</mi><mi>i</mi></msub></msqrt></mfrac><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo>+</mo><mfrac><msub><mi>ϵ</mi><mi>i</mi></msub><msqrt><msub><mi>w</mi><mi>i</mi></msub></msqrt></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\frac{y_i}{\sqrt{w_i}} = \frac{\mathbf{x}_i^\top}{\sqrt{w_i}} \mathbf{\beta} + \frac{\epsilon_i}{\sqrt{w_i}}.
</annotation></semantics></math></p>
<p>It can be shown that the solution for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><annotation encoding="application/x-tex">\mathbf{\beta}</annotation></semantics></math> is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo accent="true">̂</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><mo>,</mo></mrow><annotation encoding="application/x-tex">
\widehat{\mathbf{\beta}} = (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W} \mathbf{y},
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math> is a diagonal matrix where each entry is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mi>i</mi></msub><annotation encoding="application/x-tex">w_{i}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> is the covariate matrix, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> is the outcome column vector. Note that all weights have been scaled to sum to 1 (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>w</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_i w_{ii} = 1</annotation></semantics></math>). An easy way to get to compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{\mathbf{\beta}}</annotation></semantics></math> is to first weight both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mi>s</mi></msup><annotation encoding="application/x-tex">\mathbf{W}^s</annotation></semantics></math>, which is simply the weight matrix but using instead the square root of the weights. Let’s define these rescaled matrices as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right"><mover><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo accent="true">̃</mo></mover></mtd><mtd columnalign="left"><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mi>s</mi></msup></mtd></mtr><mtr><mtd columnalign="right"><mover><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><mo accent="true">̃</mo></mover></mtd><mtd columnalign="left"><mo>=</mo><msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mi>s</mi></msup><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
\widetilde{\mathbf{X}} &amp;= \mathbf{X} \mathbf{W}^s \\
\widetilde{\mathbf{y}} &amp;= \mathbf{W}^s \mathbf{y}
\end{aligned}
</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="heteroskedastic-consistent-variance-estimators">Heteroskedastic-consistent variance estimators<a class="anchor" aria-label="anchor" href="#heteroskedastic-consistent-variance-estimators"></a>
</h3>
<p>Turning to variance, the standard sandwich estimator is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="double-struck"><mi>𝕍</mi></mstyle><mo stretchy="false" form="prefix">[</mo><mover><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">]</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mi>Ω</mi><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
\mathbb{V}[\widehat{\mathbf{\beta}}] = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \Omega \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1}
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> represents <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="double-struck"><mi>𝔼</mi></mstyle><mo stretchy="false" form="prefix">[</mo><mstyle mathvariant="bold"><mi>𝛜</mi></mstyle><msup><mstyle mathvariant="bold"><mi>𝛜</mi></mstyle><mi>⊤</mi></msup><mo stretchy="false" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[\mathbf{\epsilon}\mathbf{\epsilon}^\top]</annotation></semantics></math>, the variance-covariance matrix of the disturbances. A nice review of the different variance estimators along with their properties can be found in <span class="citation">Long and Ervin (<a href="#ref-longervin2000" role="doc-biblioref">2000</a>)</span> <a href="http://www.indiana.edu/~jslsoc/files_research/testing_tests/hccm/99TAS.pdf" class="external-link">[ungated]</a>. The HC2 and HC3 estimators, introduced by <span class="citation">MacKinnon and White (<a href="#ref-mackinnonwhite1985" role="doc-biblioref">1985</a>)</span>, use the hat matrix as part of the estimation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>. The standard hat matrix is written:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">
\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top
</annotation></semantics></math></p>
<p>Where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">h_{ii}</annotation></semantics></math> are the diagonal elements of the hat matrix, the HC2 variance estimator is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="double-struck"><mi>𝕍</mi></mstyle><mo stretchy="false" form="prefix">[</mo><mover><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">]</mo><mrow><mi>H</mi><mi>C</mi><mn>2</mn></mrow></msub><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="normal"><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi></mstyle><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><mrow><mn>1</mn><mo>−</mo><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC2} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{1 - h_{ii}}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1}  ,
</annotation></semantics></math></p>
<p>where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>e</mi><mi>i</mi></msub><annotation encoding="application/x-tex">e_i</annotation></semantics></math> are the residuals. The HC3 estimator is very similar,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="double-struck"><mi>𝕍</mi></mstyle><mo stretchy="false" form="prefix">[</mo><mover><mstyle mathvariant="bold"><mi>𝛃</mi></mstyle><mo accent="true">̂</mo></mover><msub><mo stretchy="false" form="postfix">]</mo><mrow><mi>H</mi><mi>C</mi><mn>3</mn></mrow></msub><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="normal"><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi></mstyle><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><mrow><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><msup><mo stretchy="false" form="postfix">)</mo><mn>2</mn></msup></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mathbb{V}[\widehat{\mathbf{\beta}}]_{HC3} = (\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^\top \mathrm{diag}\left[\frac{e^2_i}{(1 - h_{ii})^2}\right] \mathbf{X} (\mathbf{X}^{\top}\mathbf{X})^{-1} .
</annotation></semantics></math></p>
<p>Both rely on the hat matrix. Crucially, this is where Stata and the packages and modules in R and Python disagree. When weights are specified, Stata estimates the hat matrix as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mrow><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbf{H}_{Stata} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top,
</annotation></semantics></math></p>
<p>while the other software uses</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mi>R</mi></msub><mo>=</mo><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo stretchy="false" form="prefix">(</mo><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><msup><mo stretchy="false" form="postfix">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mi>⊤</mi></msup><mstyle mathvariant="bold"><mi>𝐖</mi></mstyle><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mathbf{H}_{R} = \mathbf{X} (\mathbf{X}^{\top}\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{W}.
</annotation></semantics></math></p>
<p>Thus the HC2 and HC3 estimator differ as the values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">h_{ii}</annotation></semantics></math> are quite different. How different are these results? Let’s use a little example using <code>mtcars</code> a dataset included with R.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Using estimatr</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://declaredesign.org/r/estimatr/">estimatr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/lm_robust.html">lm_robust</a></span><span class="op">(</span></span>
<span>  <span class="va">mpg</span> <span class="op">~</span> <span class="va">hp</span>,</span>
<span>  data <span class="op">=</span> <span class="va">mtcars</span>,</span>
<span>  weights <span class="op">=</span> <span class="va">wt</span>,</span>
<span>  se_type <span class="op">=</span> <span class="st">"HC2"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;                Estimate Std. Error   t value     Pr(&gt;|t|)    CI Lower</span></span>
<span><span class="co">#&gt; (Intercept) 28.54864505 2.16281844 13.199742 4.975934e-14 24.13158053</span></span>
<span><span class="co">#&gt; hp          -0.06249413 0.01445662 -4.322872 1.561752e-04 -0.09201849</span></span>
<span><span class="co">#&gt;                CI Upper DF</span></span>
<span><span class="co">#&gt; (Intercept) 32.96570958 30</span></span>
<span><span class="co">#&gt; hp          -0.03296977 30</span></span></code></pre></div>
<p>We can also see that Python’s <a href="http://www.statsmodels.org/stable/index.html" class="external-link"><code>statsmodels</code></a> provides the same results as the methods in R (and in fact they note the difference in an <a href="https://github.com/statsmodels/statsmodels/issues/1209" class="external-link">issue on GitHub</a>).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3"></a>dat <span class="op">=</span> pd.read_csv(<span class="st">'mtcars.csv'</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>wls_mod <span class="op">=</span> sm.WLS(dat[<span class="st">'mpg'</span>], sm.add_constant(dat[<span class="st">'hp'</span>]), weights <span class="op">=</span> dat[<span class="st">'wt'</span>])</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="bu">print</span>(wls_mod.fit().HC2_se)</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">#&gt; const    2.162818</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">#&gt; hp       0.014457</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">#&gt; dtype: float64</span></span></code></pre></div>
<p>If we do the same in Stata 13, we get the following output:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode stata"><code class="sourceCode stata"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">insheet</span> <span class="kw">using</span> mtcars.csv</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="kw">reg</span> mpg hp [<span class="kw">aweight</span>=wt], <span class="kw">vce</span>(hc2)</span></code></pre></div>
<pre><code>Linear regression                                      Number of obs =      32
                                                       F(  1,    30) =   19.08
                                                       Prob &gt; F      =  0.0001
                                                       R-squared     =  0.5851
                                                       Root MSE      =  3.6191

------------------------------------------------------------------------------
             |             Robust HC2
         mpg |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          hp |  -.0624941   .0143083    -4.37   0.000    -.0917155   -.0332727
       _cons |   28.54865   2.155169    13.25   0.000      24.1472    32.95009
------------------------------------------------------------------------------</code></pre>
<p>Stata’s standard errors are somewhat different. The only documentation of Stata’s formula for the hat matrix can be found on the <a href="https://www.statalist.org/forums/forum/general-stata-discussion/general/329653-regress-postestimation-with-weights" class="external-link">statalist forum here</a> and nowhere in the official documentation as far as I can tell.</p>
<div class="section level4">
<h4 id="which-should-we-prefer">Which should we prefer?<a class="anchor" aria-label="anchor" href="#which-should-we-prefer"></a>
</h4>
<p>Just because Stata is not documenting their HC2 and HC3 estimator does not mean they’re wrong. Also the differences tend to be minor. In fact, it is unclear which we should prefer given that there is not a strong literature supporting one or the other. However, there are several arguments to be made for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mi>R</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_{R}</annotation></semantics></math>.</p>
<ol style="list-style-type: decimal">
<li>It’s the estimator you get when you weight your data by the square root of the weights (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo>→</mo><mover><mstyle mathvariant="bold"><mi>𝐗</mi></mstyle><mo accent="true">̃</mo></mover></mrow><annotation encoding="application/x-tex">\mathbf{X} \rightarrow \widetilde{\mathbf{X}}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><mo>→</mo><mover><mstyle mathvariant="bold"><mi>𝐲</mi></mstyle><mo accent="true">̃</mo></mover></mrow><annotation encoding="application/x-tex">\mathbf{y} \rightarrow \widetilde{\mathbf{y}}</annotation></semantics></math>) and fit regular ordinary least squares. If one considers the weighted model as simply a rescaled version of the unweighted model, then users should prefer <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mi>R</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_{R}</annotation></semantics></math>.</li>
<li>The diagonal of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mi>R</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_{R}</annotation></semantics></math> are the weighted leverages <span class="citation">(Li and Valliant <a href="#ref-livalliant2009" role="doc-biblioref">2009</a>)</span>, while <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mstyle mathvariant="bold"><mi>𝐇</mi></mstyle><mrow><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{H}_{Stata}</annotation></semantics></math> would need to be weighted again for the diagonal to recover the weighted leverage.</li>
</ol>
</div>
</div>
<div class="section level3 unnumbered">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<div id="refs" class="references">
<div id="ref-livalliant2009">
<p>Li, Jianzhu, and Richard Valliant. 2009. “Survey Weighted Hat Matrix and Leverages.” <em>Survey Methodology</em> 35 (1): 15–24.</p>
</div>
<div id="ref-longervin2000">
<p>Long, J Scott, and Laurie H Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” <em>The American Statistician</em> 54 (3): 217–24. <a href="https://doi.org/10.1080/00031305.2000.10474549" class="external-link">https://doi.org/10.1080/00031305.2000.10474549</a>.</p>
</div>
<div id="ref-mackinnonwhite1985">
<p>MacKinnon, James, and Halbert White. 1985. “Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” <em>Journal of Econometrics</em> 29 (3): 305–25. <a href="https://doi.org/10.1016/0304-4076(85)90158-7" class="external-link">https://doi.org/10.1016/0304-4076(85)90158-7</a>.</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Graeme Blair, Jasper Cooper, Alexander Coppock, Macartan Humphreys, Luke Sonnet.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
