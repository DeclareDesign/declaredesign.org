[
  {
    "objectID": "other-languages.html",
    "href": "other-languages.html",
    "title": "Declaring designs in several programming languages",
    "section": "",
    "text": "Language\nDeclaration in code\nFigure based on mock data\nDiagnosis\n\n\n\n\nR\n\n\n\n\n\nStata\n\n\n\n\n\nPython\n\n\n\n\n\nExcel\n\n\n\n\n\n\nClick on the declarations to download the code files; the figures to download the code that generated them; and the diagnosis to download a reproducible document that includes the diagnosis. You can also download the code for the reproducible documents for R, Stata, and Python.\n\nHow to compile the reproducible documents\nR: the .rmd is knit in Rstudio\nPython: compiled through pandoc using the following command:\nstitch two_arm_design_python.md -o two_arm_design_python.html\nStata: compiled from within Stata via the command:\ndyndoc two_arm_design_stata.txt, replace"
  },
  {
    "objectID": "getting-started/index.html",
    "href": "getting-started/index.html",
    "title": "Getting started with DeclareDesign",
    "section": "",
    "text": "This getting started guide is an excerpt from Chapter 4 from Blair, Coppock, and Humphreys, 2023. Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign. Princeton University Press. For a more advanced guide, see Chapter 13."
  },
  {
    "objectID": "getting-started/index.html#installing-r",
    "href": "getting-started/index.html#installing-r",
    "title": "Getting started with DeclareDesign",
    "section": "Installing R",
    "text": "Installing R\nYou can download R for free from CRAN. We also recommend the free program RStudio, which provides a friendly interface to R. Both R and RStudio are available on Windows, Mac, and Linux.\nOnce you have R and RStudio installed, open up RStudio and install DeclareDesign and its related packages. These include three packages that enable specific steps in the research process: fabricatr for simulating social science data, randomizr for random sampling and random assignment, and estimatr for design-based estimators. You can also install rdss, which includes datasets and helper functions used in the book. To install them all, copy the following code into your R console:\n\ninstall.packages(c(\"DeclareDesign\", \"rdss\"))\n\nWe also recommend that you install and get to know the tidyverse set of packages for data analysis:\n\ninstall.packages(\"tidyverse\")\n\nFor introductions to R and the tidyverse we especially recommend the free resource R for Data Science."
  },
  {
    "objectID": "getting-started/index.html#declaration",
    "href": "getting-started/index.html#declaration",
    "title": "Getting started with DeclareDesign",
    "section": "Declaration",
    "text": "Declaration\nDesigns are constructed from design elements: models, inquiries, data strategies, and answer strategies.\nIn DeclareDesign, each design element is made with a function that starts with the word declare. For example, we can declare an assignment procedure using declare_assignment as follows:\n\nlibrary(DeclareDesign)\n\nsimple_random_assignment &lt;- \n  declare_assignment(Z = simple_ra(N = N, prob = 0.6))\n\nEach element created by a declare_* function, perhaps surprisingly, is itself a function. The object simple_random_assignment is not a particular assignment — instead, it is a function that conducts assignment when called. Each time we call simple_random_assignment we get a different random assignment:\n\nparticipants &lt;- data.frame(ID = 1:100)\n\nassignment_1 &lt;- simple_random_assignment(participants)\nassignment_2 &lt;- simple_random_assignment(participants)\nassignment_3 &lt;- simple_random_assignment(participants)\n\nbind_cols(assignment_1, assignment_2, assignment_3)\n\n\n\n\n\nTable 1: Three random assignments from the same random assignment step.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1\n\n\nAssignment 2\n\n\nAssignment 3\n\n\n\nID\nZ\nID\nZ\nID\nZ\n\n\n\n\n1\n0\n1\n1\n1\n0\n\n\n2\n0\n2\n0\n2\n0\n\n\n3\n0\n3\n1\n3\n1\n\n\n4\n1\n4\n0\n4\n1\n\n\n5\n0\n5\n1\n5\n0\n\n\n\n\n\n\n\n\n\n\nEvery step in a research design can be declared using one of the declare_* functions. Table 2 collects these according to the four elements of a research design. In Chapter 13 of the book, we detail how to build each kind of step.\n\n\n\nTable 2: Declaration functions in DeclareDesign\n\n\n\n\n\n\n\n\n\n\nDesign component\nFunction\nDescription\n\n\n\n\nModel\ndeclare_model()\nbackground variables and potential outcomes\n\n\nInquiry\ndeclare_inquiry()\nresearch questions\n\n\nData strategy\ndeclare_sampling()\nsampling procedures\n\n\n\ndeclare_assignment()\nassignment procedures\n\n\n\ndeclare_measurement()\nmeasurement procedures\n\n\nAnswer strategy\ndeclare_estimator()\nestimation procedures\n\n\n\ndeclare_test()\ntesting procedures\n\n\n\n\n\n\nWe use the + operator to build from elements of a design to a design. The declaration below represents a two-arm randomized experiment with 100 units from which we aim to estimate the average treatment effect.\nTwo-arm randomized experiment\n\n\n\n\n\n\nFigure 1: Two-arm randomized experiment declaration"
  },
  {
    "objectID": "getting-started/index.html#diagnose-design-function",
    "href": "getting-started/index.html#diagnose-design-function",
    "title": "Getting started with DeclareDesign",
    "section": "Diagnosis",
    "text": "Diagnosis\nDiagnosis is the process of simulating the design many times and calculating summary statistics about the design that describe its properties, which we call diagnosands. Once a design is declared, diagnosis is as simple as using the diagnose_design function on it.\nExample design diagnosis\n\ndiagnose_design(declaration, sims = 100)\n\n\n\n\n\nTable 3: Design diagnosis.\n\n\n\n\n\n\nBias\nRMSE\nPower\n\n\n\n\n-0.02\n0.31\n0.11\n\n\n(0.03)\n(0.02)\n(0.03)\n\n\n\n\n\n\n\n\n\n\nThe output of the diagnosis includes the diagnosand values (top row), such as bias of \\(-0.01\\), and our uncertainty about the diagnosand value (bootstrapped standard error in parentheses in the bottom row). The uncertainty estimates tell us whether we have conducted enough simulations to precisely estimate the diagnosands. The fact that that the estimate of bias is \\(-0.01\\) and the standard error is \\(0.02\\) means that we cannot distinguish the amount of bias from no bias at all."
  },
  {
    "objectID": "getting-started/index.html#redesign-function",
    "href": "getting-started/index.html#redesign-function",
    "title": "Getting started with DeclareDesign",
    "section": "Redesign",
    "text": "Redesign\nWe redesign to learn how the diagnosands change as design features change. We can do this using the redesign function over a range of sample sizes, which produces a list of designs.\n\ndesigns &lt;- redesign(declaration, N = c(100, 200, 300, 400, 500))\n\nOur simulation and diagnosis tools can operate directly on this list of designs:\n\ndiagnose_design(designs)"
  },
  {
    "objectID": "getting-started/index.html#library-of-designs",
    "href": "getting-started/index.html#library-of-designs",
    "title": "Getting started with DeclareDesign",
    "section": "Library of designs",
    "text": "Library of designs\nIn our DesignLibrary package, we have created a set of common designs as designers (functions that create designs from just a few parameters), so you can get started quickly.\n\nlibrary(DesignLibrary)\n\nblock_cluster_design &lt;- \n  block_cluster_two_arm_designer(N = 1000, N_blocks = 10)"
  },
  {
    "objectID": "blog/posts/with-great-power-comes-great-responsibility.html",
    "href": "blog/posts/with-great-power-comes-great-responsibility.html",
    "title": "With great power comes great responsibility",
    "section": "",
    "text": "We usually think that the bigger the study the better. And so huge studies often rightly garner great publicity. But the ability to generate more precise results also comes with a risk. If study designs are at risk of bias and readers (or publicists!) employ a statistical significance filter, then big data might not remove threats of bias and might actually make things worse."
  },
  {
    "objectID": "blog/posts/with-great-power-comes-great-responsibility.html#bias-persists-as-n-increases",
    "href": "blog/posts/with-great-power-comes-great-responsibility.html#bias-persists-as-n-increases",
    "title": "With great power comes great responsibility",
    "section": "Bias persists as N increases",
    "text": "Bias persists as N increases\nThe figure below shows the estimated treatment effects in each simulation for a series of sample sizes. The red line is the truth — the real average treatment effect is zero in this simulation. But the estimates are all well above zero, and the problem doesn’t go away as the sample size increases.\n\nget_simulations(diagnosis) %&gt;%\n  ggplot(aes(x = N, y = estimate)) +\n    geom_point(alpha = 0.2) +\n    geom_hline(yintercept = 0.0, color = \"red\") +\n    geom_text(data = data.frame(N = 3000, estimate = -0.1, label = \"True ATE = 0\"),\n              aes(label = label)) +\n    coord_cartesian(ylim = c(-1, 3)) +\n    ggtitle(\"Big data doesn't necessarily remove bias\")"
  },
  {
    "objectID": "blog/posts/with-great-power-comes-great-responsibility.html#likelihood-of-a-false-positive-increases-as-n-increases",
    "href": "blog/posts/with-great-power-comes-great-responsibility.html#likelihood-of-a-false-positive-increases-as-n-increases",
    "title": "With great power comes great responsibility",
    "section": "Likelihood of a false positive increases as N increases",
    "text": "Likelihood of a false positive increases as N increases\nIt gets worse! As sample size increases, so does power, or the probability of getting a “statistically significant” result. The p-values will indicate that the estimate is significant even though it is badly biased away from the truth. The ATE in this simulation is zero, so power should be equal to 0.05, since that’s what we chose as our significance threshold.\n\nget_diagnosands(diagnosis) %&gt;%\n  ggplot(aes(x = N, y = power)) +\n    geom_point(alpha = 0.5) + geom_line() +\n    theme_bw() +\n    ggtitle(\"Big data does help with statistical power---even if estimates are biased\")\n\n\n\n\n\n\n\n\nIn summary, more data is generally better, but ever bigger data won’t necessarily solve the problem of unobserved confounding. A consequence of the statistical significance filter is that we tend to hear only about findings that are “significant.” Big data gets studies through this filter because, with so many observations, statistical significance is trivially easy to achieve. To be clear, we are not saying that the study in the Lancet is biased (we can’t know whether there are unobserved confounders). But if a design (observational or experimental!) is biased, increasing the sample size can make the wrong inference a whole lot more likely.\nAs Uncle Ben might have said to Peter Parker if he’d opted for social science instead of being a superhero: with great statistical power comes great inferential responsiblity."
  },
  {
    "objectID": "blog/posts/sometimes-you-need-to-cluster-standard-errors-above-level-of-treatment.html",
    "href": "blog/posts/sometimes-you-need-to-cluster-standard-errors-above-level-of-treatment.html",
    "title": "Sometimes you need to cluster standard errors above the level of treatment",
    "section": "",
    "text": "In designs in which a treatment is assigned in clusters (e.g. classrooms), it’s usual practice to account for cluster-level correlations when you generate estimates of uncertainty about estimated effects. But units often share commonalities at higher levels, such as at a block level (e.g. schools). Sometimes you need to take account of this and sometimes you don’t. We show an instance of the usual procedure of clustering by assignment cluster (classrooms) working well and show how badly you can do with a more conservative approach (clustering by schools). We then show an example of a design in which clustering at the level of treatment assignment (classroom) is not good enough; in the troublesome example, schools are thought of as being sampled from a larger population of schools and treatment effects are different in different schools. In this case, if you want estimates of uncertainty for population level effects you have to cluster at the school level even though treatment is assigned within schools.\nDavid McKenzie discussed this issue a while ago here following discussions by Chris Blattman here. While broadly in line with these discussions, our take-away leans a bit more in the direction that yes you really might want to cluster at higher levels than treatment assignment in an experiment in some cases, but only when you are targeting non-sample-based estimands such as population average effects.\nMore generally, the post shows how to check for these kinds of issues when you declare a design, exploiting the fact that with a design declared you can compare the distribution of estimates of standard errors with the standard deviation of the estimates you get across many runs. So you can see not just whether estimates of uncertainty are unbiased, but also whether they are precise."
  },
  {
    "objectID": "blog/posts/sometimes-you-need-to-cluster-standard-errors-above-level-of-treatment.html#footnotes",
    "href": "blog/posts/sometimes-you-need-to-cluster-standard-errors-above-level-of-treatment.html#footnotes",
    "title": "Sometimes you need to cluster standard errors above the level of treatment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis kind of design can also be made in a single line with the DesignLibrary package using the block_cluster_two_arm_designer().↩︎"
  },
  {
    "objectID": "blog/posts/randomization-does-not-justify-t-tests.html",
    "href": "blog/posts/randomization-does-not-justify-t-tests.html",
    "title": "Randomization does not justify t-tests. How worried should I be?",
    "section": "",
    "text": "Deaton and Cartwright (2017) provide multiple arguments against claims that randomized trials should be thought of as a kind of gold standard of scientific evidence. One striking argument they make is that randomization does not justify the statistical tests that researchers typically use. They are right in that. Even if researchers can claim that their estimates of uncertainty are justified by randomization, their habitual use of those estimates to conduct t-tests are not. To get a handle on how severe the problem is we replicate the results in Deaton and Cartwright (2017) and then use a wider set of diagnosands to probe more deeply. Our investigation suggests that what at first seems like a big problem might not in fact be so great if your hypotheses are what they often are for experimentalists—sharp and sample-focused.\nMore specifically, Deaton and Cartwright (2017) argue that that “spurious significance […] arises when the distribution of treatment effects contains outliers or, more generally, is not symmetric.” They back up the claim with simulation results from a case with heterogeneous asymmetrically distributed treatment effects that center on 0. In fact however, both the sharp null of no effect and the null of no average effect in a given sample are false in this example and so the worry about over-rejecting does not apply to these hypotheses.\nA bit more generally, under the sharp null of no effect the distribution of treatment effects in simple trials (with 50% assignment probabilities) will be perfectly symmetrical even if the distribution of potential outcomes is arbitrarily skewed and so the concern Deaton and Cartwright (2017) point to doesn’t arise in the first place.\nThe most important take away though might be that it’s hard to think through when the use of t-tests is or is not appropriate. But design diagnosis can tip you off to whether this likely a problem in your case."
  },
  {
    "objectID": "blog/posts/randomization-does-not-justify-t-tests.html#there-is-no-skew-when-sharp-nulls-are-true",
    "href": "blog/posts/randomization-does-not-justify-t-tests.html#there-is-no-skew-when-sharp-nulls-are-true",
    "title": "Randomization does not justify t-tests. How worried should I be?",
    "section": "There is no skew when sharp nulls are true",
    "text": "There is no skew when sharp nulls are true\nFor the record, we can do the same analysis when the sharp null is in fact true. This just requires replacing the potential outcomes step (step 2) in the design.\n\ndc_sharp &lt;- replace_step(dc_design, 1, \n                         declare_model(N = N, u = rlnorm(N) - exp(.5), Y_Z_0 = u, Y_Z_1 = u))\ndiagnose_design(dc_sharp)\n\n\n\nWarning in sprintf(paste0(\"%.\", digits, \"f\"), as.numeric(x)): NAs introduced by\ncoercion\n\n\n\n\n\nInquiry\nSelect\nVar Estimate\nEst Var Est\nVar Estimand\n\n\n\n\nSATE\nNA\n0.37\n0.37\n0.00\n\n\n\nNA\n(0.01)\n(0.00)\n(0.00)\n\n\nSPATE\nNA\n0.37\n0.37\n0.00\n\n\n\nNA\n(0.01)\n(0.00)\n(0.00)\n\n\n\n\n\nWe find that we do not see the same issue arise when in fact the sharp null is true (in the superpopulation, and thus in every sample). With a true sharp null (and .5 assignment probabilities), even if the potential outcomes are very skewed, the distribution of estimated effects will be symmetrical for the simple reason that, for any estimated treatment effect \\(\\hat{\\tau}\\) arising from assignment \\(Z\\), assignment \\((1-Z)\\) yields \\((-\\hat{\\tau})\\). This clarifies that the skew-based concern about over-rejecting a null that Deaton and Cartwright raise actually depends on the sharp null being false in the first place. (Though, to be clear, the assumption of 50% assignment matters here — skew in estimated effects is certainly possible under the sharp null with other assignment probabilities.)"
  },
  {
    "objectID": "blog/posts/randomization-does-not-justify-t-tests.html#but-there-can-still-be-dragons-so-then-what-to-do",
    "href": "blog/posts/randomization-does-not-justify-t-tests.html#but-there-can-still-be-dragons-so-then-what-to-do",
    "title": "Randomization does not justify t-tests. How worried should I be?",
    "section": "But there can still be dragons, so then what to do?",
    "text": "But there can still be dragons, so then what to do?\nAlthough we might not have to worry about skew when sharp nulls are true, \\(t\\)-stats might still lead you astray when tails are fat.\nAs a simple example, imagine a world in which \\(Y = 0\\) for 50 units and \\(Y = 1\\) for 50 units, independent of \\(Z\\). Say \\(Z\\) is randomly assigned to just four units. Whenever \\(Z\\) is assigned to four units with the same value on \\(Y\\), a t-test will suggest a significant difference (\\(p = 0.04\\)). You can interpret that as a claim that such a data pattern should only be observed 4% of the time if the null is true. But you can figure out pretty quickly that you will see data patterns like this about one eighth of the time.2 So the probability of observing such data under the null is actually much higher than the 4% you might infer from the \\(t\\)-test.\nSo in general there can be dangers using t-tests even with experimental data. A solution in this instance, if we are interested in a sharp null of no effect, is to do randomization inference. This would produce exactly the right p-value. But that will help only if you are alerted to the problem. To alert yourself to the problem, you could routinely diagnose a design with zero effects (a “null design”) and so set yourself up to get a tip off when your power is too high."
  },
  {
    "objectID": "blog/posts/randomization-does-not-justify-t-tests.html#footnotes",
    "href": "blog/posts/randomization-does-not-justify-t-tests.html#footnotes",
    "title": "Randomization does not justify t-tests. How worried should I be?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTake a look at the ri2 package for more on conducting randomization inference in R.↩︎\nThere is a roughly 50% chance the second unit will have the same \\(Y\\) value as the first unit, roughly 50% chance that the third will be the same as the second, and roughly 50% chance the fourth the same as the third. More exactly: (49/99)x(48/98)x(47/97) = 0.117.↩︎"
  },
  {
    "objectID": "blog/posts/pilot-studies.html",
    "href": "blog/posts/pilot-studies.html",
    "title": "Should a pilot study change your study design decisions?",
    "section": "",
    "text": "Data collection is expensive, and we often only get one bite at the apple. In response, we often conduct an inexpensive (and small) pilot test to help better design the study. Pilot studies have many virtues, including practicing the logistics of data collection and improving measurement tools. But using pilots to get noisy estimates in order to determine sample sizes for scale up comes with risks.\nPilot studies are often used to get a guess of the average effect size, which is then plugged into power calculators when designing the full study.\nThe procedure is:\nWe show in this post that this procedure turns out to be dangerous: at common true effect sizes found in the social sciences, you are at risk of selecting an underpowered design based on the noisy effect estimate in your pilot study. For a related argument about the dangers of post-hoc power analysis, which inspired this post, see Andy Gelman’s blog.\nA different procedure has better properties:\nWe show what happens in each procedure, using DeclareDesign. In each case, we’ll think about a decision the researcher wants to make based on the pilot: should I move forward with my planned study, or should I go back to the drawing board? We’ll rely on power to make that decision in the first procedure and the MDE in the second procedure.\nTo get started, we set up a designer1 for a standard two-arm trial where half of units are assigned to treatment and we use a difference-in-means estimator. We will use the same designer to simulate the pilot (N = 50) and the main study (N = 500). The designer lets us change the sample size, the true effect size, as well as the true standard deviation of the outcome (sd_y).\nWe will make use of a function that records estimates of the standard deviation of outcomes in each condition.\nsd_estimator &lt;- function(data){\n  data.frame(sd_y_0_hat = sd(data$Y[data$Z == 0]), sd_y_1_hat = sd(data$Y[data$Z == 1]))\n}\ndesigner &lt;- \n  function(sample_size = 50, true_effect_size = 0, sd_y = 1) {\n  design &lt;- \n    declare_model(\n      N = sample_size, \n      u = rnorm(N, sd = sd_y),\n      potential_outcomes(Y ~ true_effect_size * Z + u)) +\n    declare_assignment(Z = complete_ra(N)) +\n    declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n    declare_estimator(Y ~ Z, model = difference_in_means, label = \"dim\") +\n    declare_estimator(handler = label_estimator(sd_estimator), label = \"sd_estimator\")\n}"
  },
  {
    "objectID": "blog/posts/pilot-studies.html#footnotes",
    "href": "blog/posts/pilot-studies.html#footnotes",
    "title": "Should a pilot study change your study design decisions?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA designer is a function that writes designs.↩︎"
  },
  {
    "objectID": "blog/posts/neyman-sate-pate.html",
    "href": "blog/posts/neyman-sate-pate.html",
    "title": "Common estimators of uncertainty overestimate uncertainty",
    "section": "",
    "text": "Random assignment provides a justification not just for estimates of effects but also for estimates of uncertainty about effects. The basic approach, due to Neyman, is to estimate the variance in estimates of the difference between outcomes in treatment and in control outcomes using the variability that can be observed among units in control and units in treatment. It’s an ingenious approach and dispenses with the need to make any assumptions about the shape of statistical distributions or about asymptotics. The problem though is that it can sometimes be upwardly biased, meaning that it might lead you to maintain null hypotheses when you should be rejecting them. We use design diagnosis to get a handle on how great this problem is and how it matters for different estimands.\nThe key insight is that how great a problem this is depends on the target of inference. If you are targeting a sample average treatment effect (SATE), your standard errors will often be too big. If you are targeting a population average treatment effect (PATE), but that population is itself sampled from a superpopulation, your standard errors may also be too big, though the issues are less severe. If you’re targeting a superpopulation average treatment effect (SPATE), your standard errors will be the right size.\nA quick word on what we’re not talking about. Under complete random assignment (exactly \\(m\\) of \\(N\\) units in the sample are treated), the difference-in-means estimator of the average treatment effect is of course unbiased, which means that it returns exactly the right answer, on average. The bias we are talking about here isn’t in the estimator of the treatment effect, but instead, it’s in the standard error estimator.\nWe want to estimate \\(\\text{sd}\\left(\\frac{1}{n_T}\\sum_{i \\in T} Y_i(1) - \\frac{1}{n_C}\\sum_{i \\in C} Y_i(0) \\right)\\). The usual estimator for this is the Neyman standard error. This estimator uses the fact that the variance of a difference (treatment - control) can be expressed in terms of the variance of each group (variance of treatment outcomes, variance of control outcomes), and the covariance between them (see Freedman). We can estimate the variances easily enough but unfortunately we cannot estimate the covariance because we never simultaneously see outcomes for treatment and control for a given unit. Instead the Neyman estimator assumes the worst and estimates an upper bound on the variance. This is equivalent to HC2 robust standard errors in a regression setup and it is what is estimated by default in estimatr::difference_in_means and estimatr::lm_robust.\nFrom some perspectives, a conservative standard error estimator is better than the alternative. With a conservative estimator, we are less certain of our ATE estimates than we should be. If you’re conducting hypothesis tests, upward bias in the standard errors means you’re less likely to commit a Type I error (inappropriately rejecting the null) but you’re more likely to commit a Type II error (inappropriately failing to reject). So how bad this is depends on how you trade off these errors.\nWe’re going to investigate this question at three levels that correspond to three estimands. The highest level is the superpopulation, and the associated estimand is the superpopulation average treatment effect (SPATE). The next highest is the population, whose estimand is the population average treatment effect (PATE). The lowest level is the sample, with an associated estimand called the sample average treatment effect (SATE). Depending on how units are sampled into the population or the sample, these three estimands could have different values.\nWe’re interested in:\nWe can learn about all of these using a simulation conducted in DeclareDesign. The design includes the parameters r (the correlation between Y0 and Y1) and b (the superpopulation average treatment effect) in case you’d like to vary them, but we’ve set them to zero for this demonstration. Note that setting r=0 means that potential outcomes under treatment and control are not correlated. This implies heterogeneous effects: if effects were homogeneous then these outcomes would be perfectly correlated (for intuition, imagine a graph of \\(Y(1)\\) against \\(Y(0)\\)).\nr &lt;- 0 # correlation between Y0 and Y1\nb &lt;- 0 # average treatment effect\n\ndesign &lt;- \n  declare_model(N = 64,\n                u_0 = rnorm(N),\n                u_1 = rnorm(n = N, mean = r * u_0, sd = sqrt(1 - r^2)),\n                Y_Z_0 = u_0, \n                Y_Z_1 = u_1 + b) +\n  declare_sampling(S = complete_rs(N, n = 32)) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z)"
  },
  {
    "objectID": "blog/posts/neyman-sate-pate.html#footnotes",
    "href": "blog/posts/neyman-sate-pate.html#footnotes",
    "title": "Common estimators of uncertainty overestimate uncertainty",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you were interested in how the bias of the estimated variances is distributed over samples then you would want to this operation multiple times, which is easily done using sims = c(500, 1, 1, 500, 1, 1)↩︎"
  },
  {
    "objectID": "blog/posts/learning-from-p.html",
    "href": "blog/posts/learning-from-p.html",
    "title": "What does a p-value tell you about the probability a hypothesis is true?",
    "section": "",
    "text": "The humble \\(p\\)-value is much maligned and terribly misunderstood. The problem is that everyone wants to know the answer to the question: “what is the probability that [hypothesis] is true?” But \\(p\\) answers a different (and not terribly useful) question: “how (un)surprising is this evidence given [hypothesis]?” Can \\(p\\) shed insight on the question we really care about? Maybe, though there are dangers.\nThis post is inspired by conversations with @david_colquhoun who has been doing a lot of work on the misinterpretation of p-values (see especially “The false positive risk: a proposal concerning what to do about p values”). David poses the question “what is the probability that the null hypothesis is true given the observed \\(p\\)-value?” and provides a nice approach to answering this in terms of a “false positive risk.” The approach we present here is similar in spirit though based on a simulation approach given defined priors rather than being based on likelihood ratios.1\nThe key insight is that there is something to the intuition that if the world doesn’t look how it ought to look if indeed some hypothesis is right, then maybe that hypothesis isn’t right. Formally the connection comes via Bayes rule; the \\(p\\)-value (or, likelihood) plays a big role in Bayes’ formula for calculating the quantity of interest: posterior beliefs in hypotheses, given data.2 To use the rule though you need information on prior beliefs. Unfortunately, since “frequentist” statistics make no use of prior beliefs many researchers generally don’t report priors (indeed since frequentists and Bayesians think about probabilities differently, some will balk at the idea).\nBut what if you did have access to priors? With priors, you can construct Bayesian inferences from the diagnosis of a frequentist design. If we encode our priors into the population declaration then we can map from \\(p\\) to posteriors and let \\(p\\) answer the question we keep on wanting it to answer.\nHere is an illustration. Unlike most other designs we have looked at, in this design the estimand has a distribution. For simplicity we consider a design with a binary outcome; the estimand is the average treatment effect (or in epidemiology the “absolute risk increase”). The distribution for b in our model of the world reflects our beliefs about the estimand: we assume that it is distributed uniform over 0 and 1.3\nN &lt;- 50\ndesign &lt;- \n  declare_model(N = N, b = runif(1, min = 0, max = 1), u = runif(N, min = 0, max = 1),\n                potential_outcomes(Y ~ (u &lt; b)*Z + (u &gt; (1 + b)/2))) +\n  declare_assignment(Z = complete_ra(N)) +\n  declare_inquiry(ATE = b[1]) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z)\nWhen we simulate this design, each run takes a different estimand (b) from the uniform distribution, generates data and calculates effects and \\(p\\)-values.\nsimulations &lt;- simulate_design(design)\nNow if we graph the estimand from each run against the \\(p\\)-values from each run we can see the distribution of estimands conditional on the \\(p\\)-value. We can now think of each vertical slice of this graph as displaying the posterior distribution of estimands given \\(p\\).\nsimulations %&gt;%\n  ggplot(aes(y = estimand, x = p.value)) +\n    geom_point(size = 1, alpha = 0.1) +\n    stat_smooth(se = FALSE) +\n    scale_x_continuous(trans='sqrt', breaks = c(0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 1)) +\n    xlab(\"p.value (square root scale to clearly distinguish small values)\")\nWe see from the graph that a Bayesian who has access to the study design and who learns only about the \\(p\\)-value from a study should update sharply about the size of the treatment effect.4 If they see a very low \\(p\\) they should infer that the effect is large. Conversely, if they see a high \\(p\\) they should infer that the effect is probably quite small: in other words, they do infer, contrary to frequentist wisdom, that absence of evidence is evidence of absence."
  },
  {
    "objectID": "blog/posts/learning-from-p.html#footnotes",
    "href": "blog/posts/learning-from-p.html#footnotes",
    "title": "What does a p-value tell you about the probability a hypothesis is true?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDavid’s goal is more ambitious also as he is advocating for a new reporting norm for conveying the strength of evidence and so he explicitly seeks a statistic that is easy to calculate and can be calculated with a kind of common prior.↩︎\nIf \\(p\\) is the probability of the data under the null, and \\(q\\) is the quantity we care about (the probability of the null given the data), \\(\\pi\\) is the prior on the null, and \\(p'\\) and \\(\\pi'\\) are corresponding quantities for a complementary hypothesis, then Bayes rule says \\(q = p \\pi /(p \\pi + p' \\pi')\\).↩︎\nMore precisely we assume a process in which for effect \\(b\\), share \\(b\\) of the units are affected positively by treatment, share \\((1-b)/2\\) has outcome \\(Y=0\\) regardless and share \\((1-b)/2\\) has outcome \\(Y=1\\) regardless. Note that this is an informative prior—in particular it rules out the possibility of negative effects.↩︎\nIt’s easy and interesting to do the same thing to assess what one should believe about the estimand given the estimate (or given both the estimate and the \\(p\\)-value.)↩︎"
  },
  {
    "objectID": "blog/posts/improve-power-using-your-answer-strategy-not-just-your-data-strategy.html",
    "href": "blog/posts/improve-power-using-your-answer-strategy-not-just-your-data-strategy.html",
    "title": "Improve power using your answer strategy, not just your data strategy",
    "section": "",
    "text": "Most power calculators take a small number of inputs: sample size, effect size, and variance. Some also allow for number of blocks or cluster size as well as the overall sample size. All of these inputs relate to your data strategy. Unless you can control the effect size and the noise, you are left with sample size and data structure (blocks and clusters) as the only levers to play with to try to improve your power.\nIn fact, though, power depends on your answer strategy and not just your data strategy and so you might do better putting resources into improving what you do with your data rather than the amount of data you have.\n\nPower from the answer strategy\nRandom assignment generally means that you do not have to include control variables in an analysis in order to achieve unbiasedness. But including controls can improve precision and increase power. If you are trying to improve your power but adding observations is expensive, perhaps you should first explore whether you can improve power by adjusting the estimation approach.\nHere is an illustration of a two-arm trial with 40 units in which 20 units are assigned to treatment, blocking on a binary pre-treatment covariate \\(W\\). We’ll let the treatment effects vary according to \\(W\\), but the true average treatment effect (our estimand in this case) is equal to 1.\n\nN = 40\n\n# Model\nmodel &lt;- declare_model(N, W = rep(0:1, N / 2), u = rnorm(N), potential_outcomes(Y ~ 2 * Z * W + u))  \n\n# Inquiry\ninquiry     &lt;- declare_inquiry(ate = 1)  \n\n# Data strategy\nassignment   &lt;- declare_assignment(Z = block_ra(blocks = W))\nmeasurement  &lt;- declare_measurement(Y = reveal_outcomes(Y ~ Z))\n\n# Answer strategy\nestimator  &lt;- declare_estimator(Y ~ Z, estimand = \"ate\", label = \"Simple D-I-M\")\n\n# Declare the design\ndesign &lt;- model + inquiry + assignment + measurement + estimator\n\nUnder this data generating process, the treatment interacts quite strongly with \\(W\\). The average effect of treatment is 1, but the conditional average treatment effects are 0 and 2 for the two levels of \\(W\\). The difference-in-means analysis strategy for this design is equivalent to an OLS regression of the outcome on the treatment with no control variables included. Because of random assignment, this procedure is of course unbiased, but it leaves money on the table in the sense that we could achieve higher statistical power if we included information about \\(W\\) in some way. Here is the power of the difference-in-means answer strategy:\n\ndiagnose_design(design)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\nN Sims\n\n\n\n\nate\n0.32\n0.75\n0.98\n1.00\n0.32\n0.39\n0.00\n1.00\n10000\n\n\n\n\n\nSo power is good though short of conventional standards. Based on this diagnosis the probability of getting a statistically significant result is only 0.75 even though the true effect is reasonably large.\nLet’s consider two additional estimation strategies. The first controls for the pre-treatment covariate \\(W\\) in an OLS regression of the outcome on treatment plus the covariate. This strategy is the standard approach to the inclusion of covariates in experimental analysis. An alternative is the “Lin estimator,” so named by us because of the lovely description of this approach given in Lin (2013). This estimator interacts treatment with the de-meaned covariates. The lm_lin() function in the estimatr package implements the Lin estimator for easy use.\nHere is the expanded design and the diagnosis:\n\nnew_design &lt;- design +\n              declare_estimator(Y ~ Z + W,  model = lm_robust,\n                                inquiry = \"ate\", label = \"OLS: Control for W\") +\n              declare_estimator(Y ~ Z, covariates = ~ W, model = lm_lin,\n                                inquiry = \"ate\", label = \"Lin: Control + Interaction\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimator\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\nN Sims\n\n\n\n\nLin: Control + Interaction\n0.00\n0.32\n0.87\n0.95\n1.00\n0.32\n0.31\n0.00\n1.00\n10000\n\n\nOLS: Control for W\n0.00\n0.32\n0.81\n0.97\n1.00\n0.32\n0.35\n0.00\n1.00\n10000\n\n\nSimple D-I-M\n0.00\n0.32\n0.74\n0.99\n1.00\n0.32\n0.39\n0.00\n1.00\n10000\n\n\n\n\n\nWe see here a clear ranking of the three estimation strategies in terms of power. You will notice that the coverage also varies across designs: the simple difference in means approach is actually overly conservative in part because it does not take account of the blocked randomization. The OLS model that in some sense “controls for blocks” does better, but is still above the nominal coverage of 95%. In this case, the coverage of the Lin model is excellent.\n\n\nTradeoffs\nTo figure out how these gains in power from switching up estimation strategies compare with gains from increasing \\(N\\) we declare a sequence of designs, differing only in values for \\(N\\). We do that in two steps:\n\ndesigns   &lt;- redesign(new_design, N = seq(30, 80, 10))\ndiagnoses &lt;- diagnose_design(designs)\n\nThe diagnoses object now contains full diagnoses for a whole sequence of designs that assume different \\(N\\)s and that each contain multiple estimation strategies. Here is a graph of the output showing trade-offs between design size and estimation strategy.\n\ndiagnoses$diagnosands_df %&gt;%\n  ggplot(aes(N, power)) +\n  geom_line(aes(color = estimator))\n\n\n\n\n\n\n\n\nWe see here that if you had 45 units and wanted to use simple differences in means your power would be around 80%. You could up your power to just over 90% by increasing the size of the experiment to about 60 units. Or, conditional on speculations about the heterogeneous effects of treatment, you could do the same thing by staying at 45 but switching over to the Lin estimator.\n\n\nA puzzle\nSometimes researchers coarsen control variables, for example turning a 10 point democracy scale into a binary variable, because they believe the finer scale is noisier. Can you declare a design to assess whether dichotomizing an outcome variable increases or decreases power?\n\n\n\n\n\n\n\n\nReferences\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” The Annals of Applied Statistics 7 (1): 295–318."
  },
  {
    "objectID": "blog/posts/how-controlling-for-pretreatment-covariates-can-introduce-bias.html",
    "href": "blog/posts/how-controlling-for-pretreatment-covariates-can-introduce-bias.html",
    "title": "How controlling for pretreatment covariates can introduce bias",
    "section": "",
    "text": "Consider an observational study looking at the effect of a non-randomly assigned treatment, \\(Z\\), on an outcome \\(Y\\). Say you have a pretreatment covariate, \\(X\\), that is correlated with both \\(Z\\) and \\(Y\\). Should you control for \\(X\\) when you try to assess the effect of \\(Z\\) on \\(Y\\)?\nThis has been a question of some disagreement, with Rosenbaum (2002), for instance, arguing that “there is little to no reason to avoid adjustment for a true covariate, a variable describing subjects before treatment,” and Greenland, Pearl, and Robins (1999) and others arguing that you cannot answer this question without a causal model of how \\(Z\\) relates to \\(X\\) and \\(Y\\). See also this discussion hosted by Andy Gelman. Controlling for pretreatment covariates in a randomized experiment usually can’t hurt and usually increases precision – but what about in an observational study in which pretreatment covariates might be correlated with both the (nonrandomly assigned) treatment and the outcome?\nDesign declaration and diagnosis makes it relatively simple to examine the kinds of cases that worry Greenland et al and see where risks of bias might come from.\nLet’s declare a design with a model of the form:\nIn this world, \\(X\\) is a function of background unobserved variables \\(U_1\\) and \\(U_2\\), a treatment variable \\(Z\\) is a function of \\(U_2\\), and an outcome \\(Y\\) is a function of \\(Z\\) and \\(U_1\\). We can assume that that \\(X\\) is determined prior to \\(Z\\). We will use two answer strategies, both regression based approaches, one with a control and one without.1 Here is the full declaration:\n# Parameters\nb &lt;- 1\nN &lt;- 40\n\n# Declaration\nmodel &lt;- declare_model(N = N, U_1 = rnorm(N), U_2 = rnorm(N),\n                       X = U_1 + U_2, Z  = 1 * (U_2 &gt; 0),\n                       potential_outcomes(Y ~ b * Z + U_1))\ninquiry     &lt;- declare_inquiry(ate = mean(Y_Z_1 - Y_Z_0))\nmeasurement &lt;- declare_measurement(Y = reveal_outcomes(Y ~ Z))\nanswer_1    &lt;- declare_estimator(Y ~ Z, inquiry = \"ate\",\n                                 model = lm_robust, label = \"No control\")\nanswer_2    &lt;- declare_estimator(Y ~ Z + X, inquiry = \"ate\",\n                                 model = lm_robust, label = \"With control\")\n\ndesign &lt;- model + inquiry + measurement + answer_1 + answer_2\nHere is the diagnosis:\ndiagnosis &lt;- diagnose_design(design)\nDesign\nInquiry\nEstimator\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\n\n\n\n\ndesign\nate\nNo control\n-0.00\n0.31\n0.86\n0.95\n1.00\n0.31\n0.32\n0.00\n1.00\n\n\ndesign\nate\nWith control\n-1.17\n1.18\n0.14\n0.00\n-0.17\n0.17\n0.17\n0.98\n1.00\nWe see that the estimator without controls is unbiased while the one with controls is very biased. Clearly, introducing a control was a mistake, even though the control was pretreatment.\nThe reason why the estimator without controls is unbiased is simple enough: \\(U_2\\) is correlated with \\(X\\) and \\(Y\\) (via \\(Z\\)), however it is not correlated with \\(Y\\) given \\(Z\\). Indeed, in the model given, it is as if \\(Z\\) is randomly assigned by \\(U_2\\).\nThe reason why the estimator with controls behaves so poorly is not as obvious. Conditioning on \\(X\\) introduces a correlation between \\(Y\\) and \\(Z\\) that is not due to the effect of \\(Z\\) on \\(Y\\). In the language used by researchers working with graphical causal models, \\(X\\) is a “collider” for \\(U_1\\) and \\(U_2\\). Conditioning on \\(X\\) creates a “backdoor path” between \\(Z\\) and \\(Y\\), inducing a correlation between them that is not a result of the causal effect.\nFor a little more intuition, we can modify the design to look at a world in which \\(Z\\) does not affect \\(Y\\):\nnull_design &lt;- redesign(design, N = 5000, b = 0)\nDiagnosis yields:\nestimator\nbias\n\n\n\n\nNo control\n0.00\n\n\nWith control\n-1.17\nWe still observe bias. To see where this bias is coming from, let us draw data from this design and plot the relationship between \\(Z\\) and \\(Y\\). Doing so lets us see how \\(Z\\) and \\(Y\\) relate to to each other for different values of \\(X\\).\ndraw_data(null_design) %&gt;%\n  ggplot(aes(X, Y, color = as.factor(Z))) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = seq(-4, 4, 2)) +\n  scale_color_manual(values = c(\"blue\", \"red\"))\n\n\n\n\nNote: Correlation between Z and Y introduced by controlling for X.\nWhat we see here is that, overall, the red points are no higher or lower than the blue points — indicating no treatment effect. Yet within any vertical band (i.e., keeping \\(X\\) fixed), the blue dots are generally higher than the red dots. Conditioning induces a non-causal correlation.\nThe implication is quite a deep one: models of data generating processes are sometimes required in order to justify the choice of statistical models."
  },
  {
    "objectID": "blog/posts/how-controlling-for-pretreatment-covariates-can-introduce-bias.html#footnotes",
    "href": "blog/posts/how-controlling-for-pretreatment-covariates-can-introduce-bias.html#footnotes",
    "title": "How controlling for pretreatment covariates can introduce bias",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn contrast to the regression approach examined here, Bayesian approaches that model the causal structure could use information on \\(X\\) without introducing the kinds of problems presented here.↩︎"
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html",
    "href": "blog/posts/declaredesign-wizard.html",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "",
    "text": "DeclareDesign is a collection of tools to help you “declare” and “diagnose” research designs. In a word, with the DeclareDesign packages you can quickly state the core analysis-relevant features of a research design, and in return you will get a diagnosis that tells you how well your design is likely to perform and how changes in the design could improve performance.\nThat’s the good news. The bad news is that ‘DeclareDesign’ is all set up in R. Great for some, but a dealbreaker for many.\nThat’s why we created DDWizard. DDWizard is a (shiny) web interface that lets you select any design from a growing library of templates, the DesignLibrary, customize them, and interrogate them. All without ever having to write any R code.\nThe wizard has a Design tab and a Diagnose tab. The Design tab lets you load and customize a design; the Diagnose tab lets you interrogate the design—generating nice plots showing how the design performs compared to other similar designs.\nIt’s still in beta but you can already do a lot. We would love it if you tried it out and sent us feedback to improve it (see the end of post).\nTo get started immediately with the simplest design, head to https://eos.wzb.eu/ipi/DDWizard/ and load the “Two Arm” design. Click over to the Diagnose tab and select “Run diagnoses and update plot” to start learning about a very simple experiment with one treatment condition and one control condition."
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#create-a-design",
    "href": "blog/posts/declaredesign-wizard.html#create-a-design",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "1 Create a design",
    "text": "1 Create a design\nThe DeclareDesign library has templates (“designers”) for generating many common designs. In each case a complete design usually requires specifying a model of how the world works, inquiries (estimands), data strategies (sampling and assignment) and answer strategies (estimates) (see here for more detail).\nEach designer in the library can make a class of designs based on the arguments you provide. The library includes simple experimental designers, as well as designers for factorial and multiarm experiments. It also has designers for observational studies, such as regression discontinuity designs or instrumental variables designs. And it even has some designers for simple qualitative studies, such as process tracing.\nSuppose for instance you were interested in a 2-by-2 factorial design. The “Two by Two” designer in the library lets you specify the sample size, mean outcomes and standard deviations in each outcome cell, and the assignment probability for each factor (prob A, prob B). You can also specify how your estimand weights the effects across arms (weight A, weight B). Other designers allow different arguments; in the multiarm designer for example you can specify the number of arms you have and then provide arguments related to each arm. (Click the Read more button for details on each designer).\nOnce you plug in the values you want, the interface generates the code for the design and you can start putting it to work.\n\n\n\nFigure 1: The ‘Design’ tab. We have chosen a “Two by Two” factorial design (upper left panel) and selected a set of common features (lower left panel). The design code is printed in the middle panel under ‘Code output.’ In addition to viewing code you can examine sample data and view a summary of a run of the design. We can then move on to the design ‘Diagnosis’ panel. Arguments that are “fixed” in ‘Design’ tab the are hard coded (cannot be easily changed in the code or subject to variation in the ‘Diagnosis’ tab)."
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#diagnose-and-improve-your-design",
    "href": "blog/posts/declaredesign-wizard.html#diagnose-and-improve-your-design",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "2. Diagnose and improve your design",
    "text": "2. Diagnose and improve your design\nFor any design selected, you can use the diagnosis tab to further inspect design properties such as power, bias, and root mean squared error (RMSE) under different values of one or more parameters. Interactive graphs and tables make it easy to visualize trends and trade-offs between alternative design specifications.\nAs you do so you can also generate fancy figures to display your diagnoses, varying up to three design parameters at at time.\n\n\n\nFigure 2: The ‘Diagnosis’ tab. Sample graph showing how expected RMSE depends on N, the standard error, and the covariance of potential outcomes (\\(\\rho\\)).\n\n\nTo generate plots:\n\nProvide the values you want (you can give value to any parameters that are not “fixed”—see middle panel).\nChoose the parameter to appear on the x-axis and any other parameters to be displayed in the color aesthetics (optional) or along different plots (optional). (These last two options are only available when multiple parameters are being varied in the diagnosis).\nDownload the graph as .png OR download the code needed to generate the plot and the relevant data as an .R file. If you do this you can further tailor the design in any way you like."
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#share-your-design",
    "href": "blog/posts/declaredesign-wizard.html#share-your-design",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "3. Share your design",
    "text": "3. Share your design\nOnce you generate a design there are three ways to share it with others.\n\nYou can download the design as an .rds. This stores a design object that can be shared with R-speaking colleagues.\nYou can download the code for the design and share that as an ‘.R’ script. That’s a good option if you want to put a design declaration in a pre-analysis plan, for example.\nYou can send a link to your design. This is a more flexible option: Declare a design, hit the “Share” link in the top right corner, and send a colleague the URL. They should be brought back to the DDWizard and get to a page with all your design tailoring preserved. The idea here is that even if you are an R user you might want to share your design with colleagues who are not: in this case you can point them directly to your design in a way that lets them examine and interrogate it."
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#use-it-for-teaching",
    "href": "blog/posts/declaredesign-wizard.html#use-it-for-teaching",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "4. Use it for teaching",
    "text": "4. Use it for teaching\nMany designers in the DesignLibrary let you illustrate how design-based inferences depend on specific model assumptions. This can be useful for teaching. For example, the “Binary IV” design arguments can be defined in such a way that violates one or more assumptions for a strong instrumental variable. Similarly, the “Mediation” design lets you explore design properties under the violation of sequential ignorability and heterogeneous effects. Simulating and diagnosing such designs helps illustrate the magnitude and direction of bias and other properties when each assumption is violated separately, as well as what estimates are affected by or perform best under these scenarios."
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#move-beyond-the-library-use-the-ddwizard-as-an-interface-for-your-own-designs",
    "href": "blog/posts/declaredesign-wizard.html#move-beyond-the-library-use-the-ddwizard-as-an-interface-for-your-own-designs",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "5. Move beyond the library: Use the DDWizard as an interface for your own designs",
    "text": "5. Move beyond the library: Use the DDWizard as an interface for your own designs\nFor more advanced R and DeclareDesign users, the DDWizard can serve as an interactive interface for exploring the properties of any custom-made designer function.\nTo do this locally:\n\nMake a copy of the DesignLibrary repository here\nAdd new designer function(s) to the library in the \"R/\" directory of the package (tips) and rebuild the package locally;\nDownload or clone the DDWizard repository on Github here and run shiny::runApp() on the app directory.\n\nA version of the DDWizard should then run locally with new function(s) from your DesignLibrary version appearing in the dropdown menu below Choose design.\nEven better, you can use pull requests to contribute your designers to the DesignLibrary package in which case they will be available for everyone via the web app (the DDWizard is routinely updated to include the latest version of the DesignLibrary). See instructions here"
  },
  {
    "objectID": "blog/posts/declaredesign-wizard.html#feedback",
    "href": "blog/posts/declaredesign-wizard.html#feedback",
    "title": "Now there is a web interface for declaring and diagnosing research designs",
    "section": "Feedback",
    "text": "Feedback\nThe DDWizard is still under development and we want to make it better.\n\nPlease report issues and make feature requests via this google form.\nOr contribute to developing the app on our Github repository.\nOr ask questions about DeclareDesign more generally on our discussion board."
  },
  {
    "objectID": "blog/posts/declaredesign-blog.html",
    "href": "blog/posts/declaredesign-blog.html",
    "title": "DeclareDesign: The Blog",
    "section": "",
    "text": "Welcome to the DeclareDesign blog! We have been working on developing the DeclareDesign family of software packages to let researchers easily generate research designs and assess their properties. Our plan over the next six months is to put up weekly blog posts showing off features of the packages or highlighting the kinds of things you can learn about research design using this approach.\nOur very first blog post is not here but over at the World Bank Development Impact blog. Big thanks to Berk Özler and colleagues for hosting us there and helping us to spread the word. The Development Impact post introduces the ideas behind design declaration and diagnosis.\nOur first post here will use DeclareDesign to shed light on what has been a point of disagreement between researchers trying to figure out whether there are ever risks of bias arising from taking account of control variables – even “pretreatment” variables – when assessing effects in observational research."
  },
  {
    "objectID": "blog/posts/bias-cluster-randomized-trials.html",
    "href": "blog/posts/bias-cluster-randomized-trials.html",
    "title": "Cluster randomized trials can be biased when cluster sizes are heterogeneous",
    "section": "",
    "text": "In many experiments, random assignment is performed at the level of clusters. Researchers are conscious that in such cases they cannot rely on the usual standard errors and they should take account of this feature by clustering their standard errors. Another, more subtle, risk in such designs is that if clusters are of different sizes, clustering can actually introduce bias, even if all clusters are assigned to treatment with the same probability. Luckily, there is a relatively simple fix that you can implement at the design stage.\nFor intuition, imagine two clusters, one of size 1,000,000 and the other of size 1. Say that outcomes are 0 in both clusters in the control condition, but that in the treatment condition they are 0 in the big cluster and 1 in the small one. Then the average treatment effect is about 0 (really: 1/1,000,000). But depending on which cluster is assigned to treatment one will (using difference-in-means) estimate either 0 or 1. So the expected estimate is 0.5. Far from the truth.\nThe Horvitz-Thompson estimator is an alternative to difference-in-means and does not have this problem. Using Horvitz-Thompson, one would estimate either \\(\\frac{1}{10^6}\\left(\\frac{1}{0.5} - \\frac{0}{0.5}\\right)\\) or \\(\\frac{1}{10^6}\\left(\\frac{0}{0.5} - \\frac{0}{0.5}\\right)\\) and so get it right in expectation. In practice however, researchers often avoid Horvitz-Thompson since it can produce estimates outside of the ranges of the data and can exhibit high variance.\nThe design-based fix for this problem is given in Imai et al. (2009). As they show, the problem can be greatly alleviated by blocking on cluster size. We will use a simple design declaration to show the problem and how blocking helps.\n\nA design with heterogeneous cluster sizes\nLet’s declare a design with heterogeneous cluster sizes. There are 300 units in 12 clusters. Two bigger clusters are of size 100 and 10 smaller clusters are of size 10. The 200 units in clusters of size 100 have a 0 treatment effect, the other 100 in clusters of size 10 have an effect of 3. This means that the average treatment effect is 1. Note that we did not include any cluster level “shocks” though we did include heterogeneous effects by cluster.\nHere is the design:\n\nN_clusters &lt;- 12\n\ncluster_design &lt;- \n  # M: Model\n  declare_model(clusters = add_level(N = N_clusters, \n                                          cl_size = rep(c(100, 10), c(N/6, N - N/6)),\n                                          effect = ifelse(cl_size == 100, 0, 3)),\n                     units    = add_level(N = cl_size, u = rnorm(N, sd = .2),\n                                          Y_Z_0 = u, Y_Z_1 = u + effect)) +\n  \n  # I: Inquiry\n  declare_inquiry(ATE_i = mean(Y_Z_1 - Y_Z_0)) +\n\n  # D: Data Strategy\n  declare_assignment(Z = cluster_ra(clusters = clusters)) +\n  \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n\n  # A: Answer Strategy\n  declare_estimator(Y ~ Z, inquiry = \"ATE_i\", clusters = clusters,\n                    model = lm_robust, label = \"dim\") +\n  \n  declare_estimator(Y ~ Z, inquiry = \"ATE_i\", clusters = clusters,\n                    condition_prs = 0.5, simple = FALSE,\n                    model = horvitz_thompson, label = \"ht\")\n\nIn the plot below we show the distribution of possible estimates from different possible random assignments. The true treatment effect is 1. We see bias (of size 0.327) from the fact that the distribution is clearly not centered at 1. Very large effects (approx. 3) are estimated in those cases where both of the large clusters get assigned to control (and so all treated outcomes are around 3, right mode) whereas the estimated effects (approx. 0) when both are assigned to treatment are not so small in comparison (left mode), producing right skew. The HT estimator, in contrast, does fine on bias in this example and also has a tighter distribution.\n\nsimulate_design(cluster_design) %&gt;%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap( ~ estimator)\n\n\n\n\n\n\n\n\n\n\n\n\nScale matters\nThe problem in this design, we saw, arises when all the large units get assigned to the control condition. This unlucky combination is a higher probability event in small studies than it is in large studies. In larger studies we are more likely to see balance in the allocation of units to treatment and control. To illustrate, we scale up to 120 clusters rather than 12. We see a more continuous distribution of treatment effects and in a particular a shift away from the extremes of the distribution, which arise only when like types end up in like conditions.\n\nsimulate_design(redesign(cluster_design, N_clusters = 120)) %&gt;%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  facet_wrap( ~ estimator)\n\n\n\n\n\n\n\n\n\n\nIn this case the bias is of size 0.027, which is a big reduction.\n\n\nBlock on cluster size to address this risk\nHow can we address the bias in the difference-in-means estimator? As described in Imai et al. (2009), blocking our treatment assignment such that a similar number of large clusters are assigned to treatment and control, and a similar number of small clusters are assigned to treatment and control, can help a lot.\nTo see this, we can transform this cluster design into a blocked cluster design by changing the assignment strategy; here using one in which we pair off the clusters based on size and randomly assign one in each pair to treatment.\n\nmatched_cluster_design &lt;- replace_step(\n  cluster_design, step = 3, declare_assignment(Z = block_and_cluster_ra(clusters = clusters, blocks = cl_size)))\n\nThe sampling distribution of the difference-in-means estimators is tight—variation reflects only the random differences between clusters and not the systematic differences. It is, moreover, now equivalent to the HT distribution since there is essentially no differential weighting within blocks.\n\nsimulate_design(matched_cluster_design) %&gt;%\n  ggplot(aes(estimate)) + \n  geom_histogram(bins = 30) + \n  facet_wrap( ~ estimator)\n\n\n\n\n\n\n\n\n\n\n\n\nExtension: The problem is aggravated by sampling (but there is a solution in that case also!)\nThis problem is amplified when clusters are sampled from a larger population of clusters with equal probability. In this case both the difference-in-means and the Horvitz Thompson estimators will be biased. The intuition is similar to the problem with random assignment: tiny clusters are equally likely to be sampled as very large clusters, and the inclusion of some tiny clusters biases down both estimators when outcomes are a function of cluster size. This bias will exist even when randomization is blocked on cluster size. One intuitive possibility to address this bias is to change the probability of sampling to oversample large clusters and undersample small clusters – probability-proportional-to-size sampling. Higgins (2014) proposes such a design. Intuitively, this helps because those tiny clusters that lead to bias are less likely to be selected. Higgins (2014) demonstrates that the Hansen and Hurwitz (1943) estimator is unbiased in this setting.\nWhile we don’t declare this alternative design in this post we will sign off with a step declaration for the HH estimator which could be useful for addressing this problem in the future.\n\n# A step!\nhansen_hurwitz &lt;- function(data){\n  data %&gt;% group_by(clusters) %&gt;% \n      summarize(is_treated = first(Z), cluster_mean = 1/n() * sum(Y)) %&gt;% \n      ungroup %&gt;% \n      summarize(estimate = sum(1/sum(is_treated) * cluster_mean[is_treated]) -\n                           sum(1/sum(!is_treated) * cluster_mean[!is_treated])) %&gt;% \n      as.data.frame\n  }\n\n\n\n\n\n\n\n\n\nReferences\n\nHansen, Morris H., and William N. Hurwitz. 1943. “On the Theory of Sampling from Finite Populations.” The Annals of Mathematical Statistics 14 (4): 333–62.\n\n\nHiggins, Michael J. 2014. “The Benefits of Probability Proportional to Size Sampling in Cluster-Randomized Experiments.”\n\n\nImai, Kosuke, Gary King, Clayton Nall, et al. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53."
  },
  {
    "objectID": "blog/posts/a-journal-of-null-results-is-a-flawed-fix-for-a-significance-filter.html",
    "href": "blog/posts/a-journal-of-null-results-is-a-flawed-fix-for-a-significance-filter.html",
    "title": "A journal of null results is a flawed fix for a significance filter",
    "section": "",
    "text": "Mostly we use design diagnostics to assess issues that arise because of design decisions. But you can also use these tools to examine issues that arise after implementation. Here we look at risks from publication bias and illustrate two distinct types of upwards bias that arise from a “significance filter.” A journal for publishing null results might help, but the results in there are also likely to be biased, downwards.\nTwo distinct problems arise if only significant results are published:\n\nThe results of published studies will be biased towards larger magnitudes.\nThe published studies will be unrepresentative of the distribution of true effects in the relevant population of studies.\n\nThese two problems are quite distinct. The first problem is more familiar: conditional on any true effect size, larger estimates have an easier time passing the statistical significance filter, so the distribution of published results will be biased upwards because it will be missing all of the smaller estimates. The second problem is more subtle. If different studies seek to measure effects that are of different size, conditioning on statistical significance means that we are more likely to learn from places that have large effects than from places that have small effects. The significance filter means that our answers to any particular question will be biased and it means that the set of questions we see answers to will be biased as well. The Journal of Significant Results is a poor guide to the true distribution of causal effects.\nWhat about a Journal of Null Results? Such a journal would condition acceptance on failing to achieve statistical significance. The set of articles published in such a journal would also be biased. We’ll explore this idea with a quick simulation.\nThe two_arm_designer function in the DesignLibrary package generates designs for a basic two-arm trial in which, by default, half the units are assigned to treatment and the remainder to control. (See ?two_arm_designer for more details.) We’ll use this function to make a sequence of designs. We’ll vary the true value of the estimand, the average treatment effect (ate), from 0 to 1 and we’ll consider two sample sizes, N = 20 and N = 200.\n\nlibrary(DesignLibrary)\ndesigns     &lt;- expand_design(two_arm_designer, \n                             ate = seq(0, 1, 0.1), N = c(20, 200))\nsimulations &lt;- simulate_design(designs)\n\nThe data.frame simulations records results from running these designs many times—equivalent here to implementing many independent studies from a large population of possible studies. The figure shows a scatterplot of the estimand versus the estimate for each run of the study. We facet by whether we condition on significance, nonsignificance, or nothing at all.\n\n\n`summarise()` has grouped output by 'N'. You can override using the `.groups`\nargument.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nScatterplots of estimates against estimands (ranging between 0 and 1) for N = 20 and N = 200. Thick lines show mean values. If vertical and horizontal thick lines cross on the 45 degree line then estimates correspond to estimands on average.\n\n\n\n\nLooking first at the Journal of Significant Results, we see the familiar problem: the average estimate is biased away from the true value of the estimand. This problem is greatly helped by increasing the sample size. But we can also see the second problem – the distribution of estimands (the true effects under study) is also biased towards larger effects, this problem is also allayed, though less dramatically, by larger sample sizes.\nThe Journal of Null Results suffers from a parallel problem, only in reverse. Now estimands are smaller than is typical in the population and, on average, estimates are biased down relative to these estimands. Strikingly, the bias in estimand selection is worse at the larger sample size (though downwards bias within the set of published studies is smaller).\nNow, we agree that proactively publishing null results may help when considering entire research literatures as a whole, and for this reason alone a Journal of Null Results is probably a good thing.\nBut, better would be to not do any conditioning at all. The Journal of Interesting Designs would condition only on the question being interesting and the design being appropriate to answering the question. We see that the distribution of estimates and estimands are both centered on the correct average value. \n\n\nCode to produce figure\nFor those who are interested, here is the code to produce the above figure.\n\nlibrary(tidyverse)\n\nlevels &lt;- c(\"Estimate is significant \\n (Journal of Significant Results)\", \n            \"Estimate is not significant \\n (Journal of Null Results)\", \n            \"No significance filter \\n (Journal of Interesting Designs)\")\n\nsims_1 &lt;- simulations %&gt;% mutate(filter = if_else(p.value &lt; 0.05, levels[1], levels[2]))\n\nsims_2 &lt;- simulations %&gt;% mutate(filter = levels[3])\n\ngg_df &lt;- bind_rows(sims_1, sims_2) %&gt;%\n  mutate(N = paste0(\"N = \", N), filter = factor(filter, levels = levels))\n\nsummary_df &lt;- gg_df %&gt;%\n  group_by(N, filter) %&gt;%\n  summarise(mean_estimand = mean(estimand),\n            mean_estimate = mean(estimate))\n\nggplot(gg_df, aes(x = estimand, y = estimate, color = filter)) +\n  geom_point(alpha = 0.1) +\n  geom_vline(data = summary_df, aes(xintercept = mean_estimand), size = 1.2, alpha = 1, color = \"darkgrey\") +\n  geom_hline(data = summary_df, aes(yintercept = mean_estimate), size = 1.2, alpha = 1, color = \"darkgrey\") +\n  scale_colour_brewer(type = \"qual\") +\n  geom_abline() +\n  geom_vline(xintercept = 0.5) +\n  facet_grid(N ~ filter) +\n  coord_cartesian(xlim = c(-0.5, 1.5), ylim = c(-0.5, 1.5)) +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        strip.background = element_blank()) +\n  theme(panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.background = element_blank())"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About DeclareDesign",
    "section": "",
    "text": "The DeclareDesign project was founded by Graeme Blair (UCLA), Jasper Cooper (Office of Evaluation Sciences, US General Services Administration), Alexander Coppock (Yale University), and Macartan Humphreys (WZB Berlin)."
  },
  {
    "objectID": "about/index.html#publications",
    "href": "about/index.html#publications",
    "title": "About DeclareDesign",
    "section": "Publications",
    "text": "Publications\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. “Declaring and diagnosing research designs.” American Political Science Review 113, no. 3 (2019): 838-859. pdf bibtex\nBlair, Graeme, Alexander Coppock, and Macartan Humphreys. “Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign.” Princeton University Press (2023). amazon press free online bibtex"
  },
  {
    "objectID": "about/index.html#contributors",
    "href": "about/index.html#contributors",
    "title": "About DeclareDesign",
    "section": "Contributors",
    "text": "Contributors\nDeclareDesign has benefited from the contributions of many researchers and coders. We recognize key contributors in addition to the principal investigators to each component:\nDeclareDesign for R: Neal Fultz\nestimatr for R: Luke Sonnet\nfabricatr for R: Aaron Rudkin\nrandomizr for Stata: John Ternovski\nDeclareDesignWizard: Clara Bicalho, Markus Konrad, and Sisi Huang\nDesignLibrary: Clara Bicalho and Lily Medina\ndeclaredesign.org: Nicholas Rivera\nWe thank contributors to earlier iterations of the project Thomas Leavitt, Tara Slough, Georgiy Syunyaev, and Anna Wilke. We also thank early beta testers Lauren Young, Erin York, and Yang-Yang Zhou."
  },
  {
    "objectID": "about/index.html#support",
    "href": "about/index.html#support",
    "title": "About DeclareDesign",
    "section": "Support",
    "text": "Support\nWe encourage you to submit bug reports and feature requests for our software as Github issues."
  },
  {
    "objectID": "about/index.html#funding",
    "href": "about/index.html#funding",
    "title": "About DeclareDesign",
    "section": "Funding",
    "text": "Funding\nWe are grateful for major funding from the Laura and John Arnold Foundation and seed funding from EGAP – Evidence in Governance and Politics."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "DeclareDesign Blog",
    "section": "",
    "text": "Now there is a web interface for declaring and diagnosing research designs\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2020\n\n\nClara Bicalho, Sisi Huang, Markus Konrad\n\n\n\n\n\n\n\n\n\n\n\n\nAn instrument does not have to be exogenous to be consistent\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2019\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nSome designs have badly posed questions and design diagnosis can alert you to the problem\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2019\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Average Treatment Effects with Ordered Probit: Is it worth it?\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2019\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can you learn from simulating qualitative inference strategies?\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2019\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nShould a pilot study change your study design decisions?\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2019\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nUse change scores or control for pre-treatment outcomes? Depends on the true data generating process\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2019\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nA journal of null results is a flawed fix for a significance filter\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2019\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nDeclareDesign Holiday Hiatus\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes you need to cluster standard errors above the level of treatment\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nGet me a random assignment YESTERDAY\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nRandomization does not justify t-tests. How worried should I be?\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of avoiding spillovers, you can model them\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does a p-value tell you about the probability a hypothesis is true?\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nCommon estimators of uncertainty overestimate uncertainty\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nCluster randomized trials can be biased when cluster sizes are heterogeneous\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nWith great power comes great responsibility\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2018\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nHow misleading are clustered SEs in designs with few clusters?\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nThe trouble with ‘controlling for blocks’\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2018\n\n\nDeclare Design Team\n\n\n\n\n\n\n\n\n\n\n\n\nImprove power using your answer strategy, not just your data strategy\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes blocking can reduce your precision\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nYou can’t speak meaningfully about spillovers without specifying an estimand\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nHow controlling for pretreatment covariates can introduce bias\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2018\n\n\nDeclareDesign Team\n\n\n\n\n\n\n\n\n\n\n\n\nDeclareDesign: The Blog\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/badly-posed-questions.html",
    "href": "blog/posts/badly-posed-questions.html",
    "title": "Some designs have badly posed questions and design diagnosis can alert you to the problem",
    "section": "",
    "text": "An obvious requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. But we can sometimes get quite far along a research path without being conscious that the questions we ask do not have answers and the answers we get are answering different questions.\nHow could a question not have an answer? These situations can arise when inquiries depend on variables that do not exist or are undefined for some units. In this post, we’ll look at one way a question might not have an answer, but there are others.\nConsider an audit experiment that seeks to assess the effects of an email from a person indicating that they are a Democrat (versus not revealing their party identification) on whether and how well election officials respond to requests for information. Whether or not a response is sent is easily observed and measured. The quality of the response is harder to measure, though some aspects, like the tone of the response, can be measured by human coders or possibly computers if fed enough training data.\nMore difficult than the measurement problem, though, is the problem of the “tone” of responses never sent. Simply dropping such observations is no good, because of the possibility that some officials would have responded in one condition but not in the other. After all, a main purpose of audit experiments is to measure the average effect of treatment on response rates, which requires believing that at least some officials would respond in one condition but not another.\nFor that kind of subject (so called if-treated or if-untreated responders), the effect of treatment on the tone of their email is undefined. It doesn’t exist. The question, “what is the effect of treatment on tone” has no answer if, in either the treatment or control condition, the subject wouldn’t respond. That question is only well-defined for subjects who always respond, regardless of treatment.\nTo summarize:"
  },
  {
    "objectID": "blog/posts/badly-posed-questions.html#footnotes",
    "href": "blog/posts/badly-posed-questions.html#footnotes",
    "title": "Some designs have badly posed questions and design diagnosis can alert you to the problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis can of course be avoided if one is willing to speculate on what tone would be were a response given in a treatment condition by an individual who would not respond in that condition.↩︎"
  },
  {
    "objectID": "blog/posts/biased-fixed-effects.html",
    "href": "blog/posts/biased-fixed-effects.html",
    "title": "The trouble with ‘controlling for blocks’",
    "section": "",
    "text": "In many experiments, different groups of units get assigned to treatment with different probabilities. This can give rise to misleading results unless you properly take account of possible differences between the groups. How best to do this? The go-to approach is to “control” for groups by introducing “fixed-effects” in a regression set-up. The bad news is that this procedure is prone to bias. The good news is that there’s an even simpler and more intuitive approach that gets it right: estimate the difference-in-means within each group, then average over these group-level estimates weighting according to the size of the group. We’ll use design declaration to show the problem and to compare the performance of this and an array of other proposed solutions."
  },
  {
    "objectID": "blog/posts/biased-fixed-effects.html#footnotes",
    "href": "blog/posts/biased-fixed-effects.html#footnotes",
    "title": "The trouble with ‘controlling for blocks’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a well known problem. See, for instance, Angrist (1998).↩︎\nFunctions in DeclareDesign create functions that take dataframes and return dataframes or statistics. Thus, we could also have taken one draw of the data using one_draw &lt;- U() %&gt;% Y %&gt;% Z %&gt;% R. The first five variables were created by U(): block indicates the block to which the unit belongs, prob indicates the probability of assignment to treatment, tau indicates the block-level treatment effect, indiv indicates the individual ID, and e is the error term. The function Y() appends the potential outcomes, Y_Z_0 and Y_Z_1, by taking e and adding tau in treatment. The function Z() appends two variables: Z is a vector of treatment assignments, block-randomized as a function of the block probabilities, while Z_cond_prob indicates the probability that a given unit is observed in the condition to which they were actually assigned. R() reveals the potential outcomes corresponding to the assignment.↩︎"
  },
  {
    "objectID": "blog/posts/declaredesign-holiday-hiatus.html",
    "href": "blog/posts/declaredesign-holiday-hiatus.html",
    "title": "DeclareDesign Holiday Hiatus",
    "section": "",
    "text": "We’ll be back on January 7 – Happy New Year!\n\nlibrary(DeclareDesign)\nlibrary(ggplot2)\n\npop &lt;-\n  declare_model(\n    N = 14,\n    X = c(\"H\", \"A\", \"P\", \"P\", \"Y\", \" \", \n          \"H\", \"O\", \"L\", \"I\", \"D\", \"A\", \"Y\", \"S\"),\n    position = N:1,\n    index = match(X, LETTERS)\n  )\n\nggplot(data = pop(), aes(index, position, color = X)) +\n  geom_text(aes(label = X)) +\n  theme_bw() +\n  theme(legend.position = \"none\", axis.title = element_blank())"
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html",
    "title": "Get me a random assignment YESTERDAY",
    "section": "",
    "text": "You’re partnering with an education nonprofit and you are planning on running a randomized control trial in 80 classrooms spread across 20 community schools. The request is in: please send us a spreadsheet with random assignments. The assignment’s gotta be blocked by school, it’s gotta be reproducible, and it’s gotta be tonight. The good news is that you can do all this in a couple of lines of code. We show how using some DeclareDesign tools and then walk through handling of more complex cases."
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html#incorporate-known-data-into-a-dataframe-before-randomization",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html#incorporate-known-data-into-a-dataframe-before-randomization",
    "title": "Get me a random assignment YESTERDAY",
    "section": "Incorporate known data into a dataframe before randomization",
    "text": "Incorporate known data into a dataframe before randomization\nIn general the world is not quite as neat as in the example above. Say in fact our data came in like this (of course it would be easier if you were sent a nice dataset but you cannot always count on that):\n\nJust got the numbers: School 1 has 6 classes, school 2 has 8, schools 4 and 8 have 3 classes, schools 5 and 9 have 5, school 6 has 2, school 10 has 7. Remember we need this yesterday.\n\nWe want to incorporate this information in the data fabrication step. The add_level functionality in the fabricate function is key here for respecting the multilevel nature of the dataset.\n\nmake_data &lt;- declare_model(\n  school = add_level(N = 10),\n  class  = add_level(N = c(6, 8, 4, 3, 5, 2, 4, 3, 5, 7)))\n\nThis new function would generate a data set that reflects the number of classes in each school."
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html#make-better-blocks-from-richer-data",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html#make-better-blocks-from-richer-data",
    "title": "Get me a random assignment YESTERDAY",
    "section": "Make better blocks from richer data",
    "text": "Make better blocks from richer data\nSay that you had even richer data about the sampling frame. Then you could use this to do even better assignments. Say this is what you get from your partner:\n\nWe just got information on the class sizes. Here it is: School 1: 20, 25, 23, 30, 12, 15; School 2: 40, 42, 53, 67, 35, 22, 18, 18; School 3: 34, 37, 28, 30; School 4: 18, 24, 20; School 5: 10, 24, 13, 26, 18; School 6: 20, 25; School 7: 28, 34, 19, 24; School 8: 32, 25, 31; School 9: 23, 20, 33, 22, 35; School 10: 20, 31, 34, 35, 18, 23, 22\n\nWe want to incorporate this information in the data fabrication step.\n\nmake_data &lt;- declare_model(\n    school = add_level(N = 10),\n    class = add_level(N = c(6, 8, 4, 3, 5, 2, 4, 3, 5, 7),\n                size = c(\n                  20, 25, 23, 30, 12, 15,         # students in each classroom of school 1\n                  40, 42, 53, 67, 35, 22, 18, 18, # students in each classroom of school 2\n                  34, 37, 28, 30,                 # etc...\n                  18, 24, 20,\n                  10, 24, 13, 26, 18,\n                  20, 25,\n                  28, 34, 19, 24,\n                  32, 25, 31,\n                  23, 20, 33, 22, 35,\n                  20, 31, 34, 35, 18, 23, 22\n                )))\n\nThis new data on class sizes might be very useful.\nIf the NGO wants to examine individual level outcomes but is using a classroom-level intervention, then this design is using “clustered” random assignment (students clustered into classrooms). As noted in Imai et al. (2009), however, if clusters are of uneven sizes, standard estimation approaches can be biased, even though it’s a randomized experiment. They propose blocking on cluster size in order to address this problem. We’ll use blockTools (by Ryan T. Moore and Keith Schnakenberg) to block on both school and classroom size within schools.\nThe block function within blockTools forms blocks as a function of size by pairing similarly-sized classrooms (blocks of size 2 is the default for the block function, though this can be changed). By telling the function that classes are grouped within schools, we can ensure that blockTools will pair classrooms within the same school. We turn this functionality into a design step like this:\n\n# A step to generate blocks\nblock_function &lt;- function(data) {\n    out &lt;- block(data, id.vars = \"class\", groups = \"school\", block.vars = \"size\")\n    mutate(data, block_id = createBlockIDs(out, data, id.var = \"class\"))}\n\nmake_blocks &lt;- declare_step(handler =  block_function)\n\nLet’s make a design that includes this step and now uses block_id instead of class for blocking:\n\ndesign &lt;- make_data + make_blocks + declare_assignment(Z = block_ra(blocks = block_id))\n\nWhen we draw data we now get the block variables and the assignment probabilities included. One out of two units is assigned to each block of two and singleton blocks are assigned independently with 0.5 probability to treatment.\n\nour_df &lt;- draw_data(design)\n\n\n\n\n\n\n\nschool\nclass\nsize\nblock_id\nZ\n\n\n\n\n01\n01\n20\n3\n0\n\n\n01\n02\n25\n1\n1\n\n\n01\n03\n23\n1\n0\n\n\n01\n04\n30\n3\n1\n\n\n01\n05\n12\n2\n0\n\n\n01\n06\n15\n2\n1\n\n\n02\n07\n40\n5\n0\n\n\n02\n08\n42\n5\n1\n\n\n02\n09\n53\n7\n0\n\n\n02\n10\n67\n7\n1\n\n\n02\n11\n35\n6\n1\n\n\n02\n12\n22\n6\n0\n\n\n02\n13\n18\n4\n0\n\n\n02\n14\n18\n4\n1\n\n\n03\n15\n34\n9\n0\n\n\n03\n16\n37\n9\n1\n\n\n03\n17\n28\n8\n1\n\n\n03\n18\n30\n8\n0\n\n\n04\n19\n18\n10\n1\n\n\n04\n20\n24\n11\n1\n\n\n04\n21\n20\n10\n0\n\n\n05\n22\n10\n13\n0\n\n\n05\n23\n24\n12\n0\n\n\n05\n24\n13\n13\n1\n\n\n05\n25\n26\n12\n1\n\n\n05\n26\n18\n14\n1\n\n\n06\n27\n20\n15\n0\n\n\n06\n28\n25\n15\n1\n\n\n07\n29\n28\n16\n1\n\n\n07\n30\n34\n17\n1\n\n\n07\n31\n19\n17\n0\n\n\n07\n32\n24\n16\n0\n\n\n08\n33\n32\n18\n0\n\n\n08\n34\n25\n19\n0\n\n\n08\n35\n31\n18\n1\n\n\n09\n36\n23\n20\n0\n\n\n09\n37\n20\n22\n1\n\n\n09\n38\n33\n21\n0\n\n\n09\n39\n22\n20\n1\n\n\n09\n40\n35\n21\n1\n\n\n10\n41\n20\n25\n0\n\n\n10\n42\n31\n26\n1\n\n\n10\n43\n34\n23\n1\n\n\n10\n44\n35\n23\n0\n\n\n10\n45\n18\n25\n1\n\n\n10\n46\n23\n24\n1\n\n\n10\n47\n22\n24\n0\n\n\n\n\n\n\n\n\nThe graph below shows that indeed, the blocking formed pairs of classrooms within each school that are similar to one another and correctly left some classrooms in a block by themselves when there was an odd number of classrooms in a school.\n\nour_df %&gt;%\n  ggplot(aes(size, block_id, color = as.factor(Z))) +\n  geom_point() +\n  facet_wrap(~school, scales = \"free_y\", ncol = 5)"
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html#replicate-yourself",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html#replicate-yourself",
    "title": "Get me a random assignment YESTERDAY",
    "section": "Replicate yourself",
    "text": "Replicate yourself\nIf you set up your assignments as functions like this then it is easy to implement the assignment multiple times:\n\nmany_assignments &lt;- replicate(1000, draw_data(design))\n\nIt’s useful to preserve a collection of assignments like this. One advantage is that it lets you see directly what the real assignment probabilities are—not just the intended assignment probabilities. This is especially useful in complex randomizations where different units might get assigned to treatment with quite different probabilities depending on their characteristics (for instance a procedure that rejects assignment profiles because of concerns with imbalance). Calculating the actual probabilities lets you figure out if you have set things up correctly and in some cases can even be useful for correcting things at the analysis stage if you have not!1 In addition the collection of assignments stored in many_assignment are exactly the assignments you should use for some randomization inference tests."
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html#save-the-output-to-an-excel-spreadsheet",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html#save-the-output-to-an-excel-spreadsheet",
    "title": "Get me a random assignment YESTERDAY",
    "section": "Save the output to an excel spreadsheet",
    "text": "Save the output to an excel spreadsheet\nYour partners mightn’t love csv spreadsheets. But you can easily save to other formats.\nMany people love Microsoft Excel. Using the writexl package (by Jeroen Ooms) will make them happy:\n\nlibrary(writexl)\n\nset_seed(20181204)\n\nour_df &lt;- draw_data(design)\n\nwrite_xlsx(our_df, path = \"students_with_random_assignment.xlsx\")"
  },
  {
    "objectID": "blog/posts/give-me-a-random-assignment-yesterday.html#footnotes",
    "href": "blog/posts/give-me-a-random-assignment-yesterday.html#footnotes",
    "title": "Get me a random assignment YESTERDAY",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome tools in the randomizr package make it even easier to obtain matrices of permutations and to learn about properties of your assignments. Using the randomizr package you can declare a randomization like this ra_declaration &lt;- declare_ra(blocks = class) and then get a print out of features of the assignments using summary(ra_declaration) and a whole matrix of assignments like this perm_mat &lt;- obtain_permutation_matrix(ra_declaration)↩︎"
  },
  {
    "objectID": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html",
    "href": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html",
    "title": "How misleading are clustered SEs in designs with few clusters?",
    "section": "",
    "text": "Cluster-robust standard errors are known to behave badly with too few clusters. There is a great discussion of this issue by Berk Özler “Beware of studies with a small number of clusters” drawing on studies by Cameron, Gelbach, and Miller (2008). See also this nice post by Cyrus Samii and a recent treatment by Esarey and Menger (2018). A rule of thumb is to start worrying about sandwich estimators when the number of clusters goes below 40. But here we show that diagnosis of a canonical design suggests that some sandwich approaches fare quite well even with fewer than 10 clusters.\nWe’ll explore this question by looking at a range of cluster randomized trials. We do this by generating a base design which includes the number of clusters and the number of units per cluster as arguments and then using that design to make a sequence of designs that vary these arguments. In this design we draw separate errors at the individual and cluster levels so that outcomes are correlated within the clusters. Specifically, we assume a fairly large “intracluster correlation coefficient” (ICC) of 0.5. The design employs a range of different approaches for estimating standard errors from the estimatr package, alongside a naive approach that ignores the clusters entirely.1\nHere’s the basic design:\nN_clusters &lt;- 4\ncluster_size &lt;- 5\n\ncluster_design &lt;- \n  \n    declare_model(\n      clusters = add_level(N = N_clusters,   u_c = rnorm(N, sd = .707)),\n      units    = add_level(\n        N = cluster_size, \n        u_i = rnorm(N, sd = .707),\n        potential_outcomes(Y ~ u_i + u_c))\n    ) +\n      \n    declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n      \n    # cluster assignment\n    declare_assignment(Z = cluster_ra(clusters = clusters)) +\n      \n    declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n      \n    # analysis, with different approaches to clustering\n    declare_estimator(Y ~ Z, model = lm_robust,  \n                        inquiry = \"ATE\", label = \"1: Naive\") + \n\n    declare_estimator(Y ~ Z, model = lm_robust, clusters = clusters, \n                        se_type = \"stata\", inquiry = \"ATE\", label = \"2: stata\") +\n      \n    declare_estimator(Y ~ Z, model = lm_robust, clusters = clusters, \n                        se_type = \"CR0\", inquiry = \"ATE\", label = \"3: CR0\") +\n\n    declare_estimator(Y ~ Z, model = lm_robust, clusters = clusters, \n                        se_type = \"CR2\", inquiry = \"ATE\", label = \"4: CR2\")\nWe use the redesign function to make a sequence of related designs based on the base design.2 We’re especially interested in what happens at small numbers of clusters, since that’s where the trouble lies, so we will focus the sequence on the low end.\ncluster_designs &lt;- redesign(cluster_design,\n                            N_clusters = c(4, 5, 6, 7, 8, 9, 10, 20, 30), \n                            cluster_size = c(3, 30))\nWe diagnose all of these in one go:\ndiagnosis &lt;- diagnose_design(cluster_designs, sims = sims)\nLet’s now graph the output separately for the expected standard error, power, and coverage."
  },
  {
    "objectID": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#standard-errors",
    "href": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#standard-errors",
    "title": "How misleading are clustered SEs in designs with few clusters?",
    "section": "Standard Errors",
    "text": "Standard Errors\nOur first plot compares the true standard error (the standard deviation of the estimates themselves) to the expected standard error estimate. The blue points are the true standard errors at each sample size; they go down as the number of clusters increases. The red points are the average estimated standard errors. When the number of clusters is small, we see that the average estimate is too small: the standard error estimators are downwardly biased. This problem is extreme for the naive approach. It is still clearly an issue for “CR0” (a variant of cluster-robust standard errors that appears in R code that circulates online) and Stata’s default standard errors. We see though that it is not as severe for the CR2 standard errors (a variant that mirrors the standard HC2 robust standard errors formula). We’re using the adjustment described in Pustejovsky and Tipton (2018).\n\nget_diagnosands(diagnosis) %&gt;%\n  gather(diagnosand, value, sd_estimate, mean_se) %&gt;%\n  ggplot(aes(N_clusters, value, group = diagnosand, color = diagnosand)) +\n  geom_point() + geom_line() +\n  theme(legend.position = \"bottom\", strip.background = element_blank()) +\n  facet_grid(cluster_size ~ estimator, labeller = label_both)"
  },
  {
    "objectID": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#power",
    "href": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#power",
    "title": "How misleading are clustered SEs in designs with few clusters?",
    "section": "Power",
    "text": "Power\nIn our data-generating process, the true ATE is exactly zero. Statistical power is the probability of getting a significant estimate. Since the true ATE is exactly zero, this probability should be exactly 0.05, as we’re using the standard significance threshold. Just as the analysis of the standard errors showed, when the number of clusters is small, we’re anticonservative. The naive approach is again wildly off, particularly when there are large clusters. But the clustered approaches also have problems. When the number of clusters is smaller than 10, the CR0 and Stata estimators are falsely rejecting at rates exceeding 10%.\n\nget_diagnosands(diagnosis) %&gt;%\nggplot(aes(N_clusters, power)) +\n  geom_point() + geom_line() +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\") +\n  theme(strip.background = element_blank()) +\n  facet_grid(cluster_size ~ estimator, labeller = label_both)"
  },
  {
    "objectID": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#coverage",
    "href": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#coverage",
    "title": "How misleading are clustered SEs in designs with few clusters?",
    "section": "Coverage",
    "text": "Coverage\nCoverage is the rate at which the estimated confidence intervals include the true value of the parameter. We’re estimating 95% confidence intervals, so if things are working as advertised, coverage would be 95%. But since at small numbers of clusters, we’re overconfident (the standard errors are too small), the coverage rates are well below the 95% target. Again though, CR2 performs quite well.\n\nget_diagnosands(diagnosis) %&gt;%\nggplot(aes(N_clusters, coverage)) +\n  geom_point() + geom_line() +\n  geom_hline(yintercept = 0.95, linetype = \"dashed\") +\n  theme(strip.background = element_blank()) +\n  facet_grid(cluster_size ~ estimator, labeller = label_both)"
  },
  {
    "objectID": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#footnotes",
    "href": "blog/posts/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html#footnotes",
    "title": "How misleading are clustered SEs in designs with few clusters?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBig thanks to Luke Sonnet who played a huge role in developing this functionality in estimatr.↩︎\nThe redesign function changes the values of arguments that are given explicitly in step declaration. See ? redesign for examples where redesign does not change the values of arguments that are used by a design but not given explicitly when steps are declared.↩︎"
  },
  {
    "objectID": "blog/posts/instrument-does-not-have-to-be-exogenous.html",
    "href": "blog/posts/instrument-does-not-have-to-be-exogenous.html",
    "title": "An instrument does not have to be exogenous to be consistent",
    "section": "",
    "text": "We often think of an instrumental variable (\\(Z\\)) as a random shock that generates exogenous variation in a treatment of interest \\(X\\). The randomness of \\(Z\\) lets us identify the effect of \\(X\\) on \\(Y\\), at least for units for which \\(Z\\) perturbs \\(X\\) in a way that’s not possible by just looking at the relationship between \\(X\\) and \\(Y\\). But surprisingly, we think, if effects are constant the instrumental variables estimator can be consistent for the effect of \\(X\\) on \\(Y\\) even when the relationship between the instrument (\\(Z\\)) and the endogenous variable (\\(X\\)) is confounded (for example, Hernán and Robins 2006). That’s the good news. Less good news is that when there is effect heterogeneity you can get good estimates for some units but it can be hard to know which units those are (Swanson and Hernán 2017). We use a declaration and diagnosis to illustrate these insights.\nConsider the causal graph below (a directed acyclic graph, or DAG). Here \\(U_2\\) confounds the relationship between \\(X\\) and \\(Y\\). That’s the usual thing and the reason for why you need something like an instrument in the first place. But now a binary \\(U_1\\) also confounds the relationship between \\(Z\\) and \\(X\\) (see for instance Fig 2d Swanson et al. (2018)). To emphasize, in this set up, the instrumental variable and the instrumented variable share a common cause in \\(U_1\\).\nDAG for a non-randomized instrument. Z is an instrument for the effect of X on Y\nUsing DeclareDesign, we write down a declaration that is consistent with this DAG. Our declaration specifies some “compliance types” or “principal strata.” In the standard IV setup with a binary instrument \\(Z\\) and a binary treatment \\(X\\), there are only four types (corresponding to all conceivable ways that the endogenous variable can respond to the exogenous variable): compliers, never-takers, always-takers, and defiers.1 But here, the binary \\(U_1\\) and binary \\(Z\\) both affect \\(X\\), so there are actually sixteen conceivable ways to comply!2\nWe simplify by imagining a world with only three types: \\(U_1\\)-compliers, \\(Z\\)-compliers, and never-takers.3 \\(U_1\\)-compliers have \\(X = 1\\) if and only if \\(U_1 = 1\\). \\(Z\\)-compliers have \\(X = 1\\) if and only if \\(Z = 1\\). Never-takers have \\(X = 0\\) regardless of the values of \\(U_1\\) or \\(Z\\). \\(U_2\\) takes three values, 1-3, and plays two roles. First, it determines whether you are a \\(U_1\\)-complier, \\(Z\\)-complier, or never-taker with respect to \\(X\\). Second, it is positively correlated with your value in \\(Y\\).\nWe’ll define five estimands, which is a lot.\nWe’ll use three estimators:\nThe declaration also includes two additional parameters (bear with us!) that we’ll vary down below. They are:\nOK, here’s the design:\nheterogeneity &lt;- 0   # how much does LATE_Z differ from LATE_U1?\npath_weight   &lt;- .5  # what fraction of compliers are U1 compliers?\n\ndesign &lt;-\n  \n  # U2 is the compliance \"type\" (U1 complier, Z complier, or Never Taker)\n  declare_model(\n    N = 2000, \n    # U1 is binary\n    U1 = complete_ra(N),\n    U2 = complete_ra(\n      N, \n      prob_each = c(path_weight * 0.6, (1 - path_weight) * 0.6, 0.4), \n      conditions = c(\"U1_c\", \"Z_c\", \"NT\")),\n    noise = rnorm(N),\n    # X potential outcomes in terms of U1 and Z, given U2 (there are **four**)\n    potential_outcomes(X ~ as.numeric((U2 == \"U1_c\") * U1 +\n                                        (U2 == \"Z_c\")  * Z),\n                       conditions = list(U1 = c(0, 1), Z = c(0, 1))),\n    # X potential outcomes in terms of Z given U1 and U2 (there are **two**)\n    potential_outcomes(X ~ as.numeric((U2 == \"U1_c\") * U1 +\n                                        (U2 == \"Z_c\")  * Z)),\n    # Y potential outcomes in terms of X (there are **two**)\n    potential_outcomes(Y ~ X * (1 + heterogeneity * as.numeric(U2)) + noise, \n                       conditions = list(X = c(0, 1))),\n    # Y potential outcomes in terms of Z (there are **two**)\n    # this is same as above, but X is written in terms of Z\n    potential_outcomes(Y ~ as.numeric((U2 == \"U1_c\") * U1 + (U2 == \"Z_c\") * Z) * \n                         (1 + heterogeneity * as.numeric(U2)) + noise)) +\n  \n  # Z is affected by U1: among U1 = 0, Z has probability 0.3; among U1 = 1, probability 0.6\n  declare_assignment(Z = block_ra(blocks = U1, block_prob = c(0.3, 0.6))) +\n  \n  declare_measurement(X = reveal_outcomes(X ~ U1 + Z)) +\n  \n  # reveal Y (doesn't matter \"how\" this is revealed, either via the X pos or the Z pos)\n  declare_measurement(Y = reveal_outcomes(Y ~ X)) +\n  \n  # Use X POs to learn about the two complier definitions\n  declare_measurement(\n    U1_complier = X_U1_0_Z_0 == 0 & X_U1_0_Z_1 == 0 & X_U1_1_Z_0 == 1 & X_U1_1_Z_1 == 1,\n    Z_complier  = X_U1_0_Z_0 == 0 & X_U1_1_Z_0 == 0 & X_U1_0_Z_1 == 1 & X_U1_1_Z_1 == 1) +\n  \n  # 5 (!) estimands\n  declare_inquiry(\n    ATE =         mean(Y_X_1 - Y_X_0),\n    LATE_U1 =     mean(Y_X_1[U1_complier] - Y_X_0[U1_complier]),\n    LATE_Z =      mean(Y_X_1[Z_complier] - Y_X_0[Z_complier]),\n    ITT =         mean(Y_Z_1 - Y_Z_0),\n    first_stage = mean(X_Z_1) - mean(X_Z_0)) + \n  \n  # Three estimators\n  declare_estimator(Y ~ Z, inquiry = \"ITT\", model = lm_robust, label =  \"itt\") +\n  declare_estimator(X ~ Z, inquiry = \"first_stage\", model = lm_robust, label =  \"first\") +\n  declare_estimator(Y ~ X | Z, inquiry = c(\"ATE\", \"LATE_U1\", \"LATE_Z\"), \n                    model = iv_robust, label =  \"iv_robust\")\nLet’s now diagnose this design to show that – pretty counterintuitively for us at least! – the IV estimator still recovers the LATE even when \\(Z\\) and \\(X\\) have a common cause.\ndiagnosands &lt;- \n  declare_diagnosands(bias = mean(estimate - estimand),\n                      mean_estimate = mean(estimate),\n                      mean_estimand = mean(estimand))\ndiagnosis_1 &lt;- \n  diagnose_design(\n    design, diagnosands = diagnosands)\nDesign\nInquiry\nEstimator\nTerm\nN Sims\nBias\nMean Estimate\nMean Estimand\n\n\n\n\ndesign\nATE\niv_robust\nX\n200\n-0.01\n0.99\n1.00\n\n\n\n\n\n\n\n(0.01)\n(0.01)\n(0.00)\n\n\ndesign\nfirst_stage\nfirst\nZ\n200\n0.09\n0.39\n0.30\n\n\n\n\n\n\n\n(0.00)\n(0.00)\n(0.00)\n\n\ndesign\nITT\nitt\nZ\n200\n0.09\n0.39\n0.30\n\n\n\n\n\n\n\n(0.00)\n(0.00)\n(0.00)\n\n\ndesign\nLATE_U1\niv_robust\nX\n200\n-0.01\n0.99\n1.00\n\n\n\n\n\n\n\n(0.01)\n(0.01)\n(0.00)\n\n\ndesign\nLATE_Z\niv_robust\nX\n200\n-0.01\n0.99\n1.00\n\n\n\n\n\n\n\n(0.01)\n(0.01)\n(0.00)\nLet’s take the diagnosis estimand-by-estimand:\nYou see in the estimator column that the bias in the ITT and first stage estimates are proportionate, such that, in the ratio, the bias cancels out. We provide some intuition for this result below, where we show how it can also break down.\nThe upshot is that when assessing the validity of IV you might be imposing an additional assumption that you might not need to impose. This may be good news in some specific research settings where the more stringent assumption of exogenous assignment of \\(X\\) to \\(Z\\) is not well justified.4"
  },
  {
    "objectID": "blog/posts/instrument-does-not-have-to-be-exogenous.html#footnotes",
    "href": "blog/posts/instrument-does-not-have-to-be-exogenous.html#footnotes",
    "title": "An instrument does not have to be exogenous to be consistent",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDenoting \\(x\\) the value that the endogenous variable \\(X\\) takes, compliers have \\(x = f_X(Z) = Z\\), never-takers have \\(x = f_X(Z) = 0\\), always-takers have \\(x = f_X(Z) = 1\\), and defiers have \\(x = f_X(Z) = (1 - Z)\\).↩︎\nThe value \\(X\\) takes could depend on all four potential combinations of \\(U_1\\) and \\(Z\\). So if \\(x = f_X(U_1,Z)\\), you have to think about every possible way in which \\(f_X(0,0)\\), \\(f_X(1,0)\\), \\(f_X(0,1)\\) and \\(f_X(0,0)\\) can map into values of \\(x\\).↩︎\nThe assumption that there are no types who “defy” their \\(U_1\\) or \\(Z\\) assignments amounts to a monotonicity assumption common in the IV framework.↩︎\nOf course, the result also holds when \\(Z\\) and \\(X\\) are unconfounded, which also allows for unbiased estimation of the ITT and the first stage, which may be of practical or theoretical interest in and of themselves.↩︎\nMore precisely, weaker conditions than homogeneity can be sufficient for identification. As put by Hernán and Robins (2006), a sufficient assumption is that “the X-Y causal risk difference is the same among treated subjects with \\(Z=1\\) as among treated subjects with \\(Z=0\\), and similarly among untreated subjects.” See other conditions discussed in Swanson et al. (2018).↩︎"
  },
  {
    "objectID": "blog/posts/modelling-spillovers.html",
    "href": "blog/posts/modelling-spillovers.html",
    "title": "Instead of avoiding spillovers, you can model them",
    "section": "",
    "text": "Spillovers are often seen as a nuisance that lead researchers into error when estimating effects of interest. In a previous post, we discussed sampling strategies to reduce these risks. A more substantively satisfying approach is to try to study spillovers directly. If we do it right we can remove errors in our estimation of primary quantities of interest and learn about how spillovers work at the same time.\nThis is important because spillovers are part of the overall effects of an intervention. If vaccination of one subject improves health outcomes for another, then this gain needs to be understood to assess the total benefits of vaccination. This is hard because we are not used to thinking of spillover effects as estimands, or to writing them in terms of potential outcomes, and so we generally do not think in terms of bias or power for spillover effects. But we can."
  },
  {
    "objectID": "blog/posts/modelling-spillovers.html#footnotes",
    "href": "blog/posts/modelling-spillovers.html#footnotes",
    "title": "Instead of avoiding spillovers, you can model them",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs with any design from the library, you can always look at the underlying code using get_design_code(spillover_designer()).↩︎\nThis dgp function differs from what we examined before by having spillovers that are not complete: direct effects differ from indirect effects.↩︎\nIn fact if you look a the design declaration we are a little more thorough and define the estimand in terms of the treatment status of all individuals and not just the in-group members.↩︎"
  },
  {
    "objectID": "blog/posts/ordered-probit.html",
    "href": "blog/posts/ordered-probit.html",
    "title": "Estimating Average Treatment Effects with Ordered Probit: Is it worth it?",
    "section": "",
    "text": "We sometimes worry about whether we need to model data generating processes correctly. For example you have ordinal outcome variables, on a five-point Likert scale. How should you model the data generation process? Do you need to model it at all? Go-to approaches include ordered probit and ordered logit models which are designed for this kind of outcome variable. But maybe you don’t need them. After all, the argument that the difference-in-means procedure estimates the treatment effect doesn’t depend on any assumptions about the type of data (as long as expectations are defined)—ordered, count, censored, etc. We diagnose a design that hedges by using both differences in means and an ordered probit model. We do so assuming that the ordered probit model correctly describes data generation. Which does better?\nFor the design declaration we assume that the estimand of interest is the average treatment effect on the scale of the data. We do this, perhaps contentiously, even though our outcome is defined on a 1 to 5 ordinal scale. The estimand can be thought of as the average number of steps up or down induced by treatment. This estimand has to be interpreted cautiously, however. In particular it presupposes an interest in outcomes in terms of “steps.” But the “size” of steps depends on the categories provided. Adding in an extra option in the middle of a scale, for example, changes the range of the scale and changes the value of the estimand. Given these issues, other estimands are possible and perhaps preferable (Volfovsky, Airoldi, and Rubin 2015), but for this design, we’ll focus on the ATE.\nSurprisingly, perhaps, we find no real difference between the model-based ordinal logit approach and the differences in means approach. The ordered probit is much more complicated to implement and to draw inferences from: the model is estimated using maximum likelihood, simulation is then used to generate a distribution of fitted values, from which a simulation of the distribution of treatment effects is produced. The standard deviation of these differences is used as an estimate of the standard deviation of treatment effects. The Zelig package (Choirat et al. 2018) makes it easy to do all this quite compactly and turn it all into a step in our design.\nWe call the zelig library, set some parameters, and define a function to take data in and return results.\nlibrary(Zelig)\nlibrary(ZeligChoice)\n\n# This helper function implements an ordered probit model using zelig; calculates \n# quantities of interest and return tidy output\n\nordered_probit &lt;-\n  \n  function(data) {\n    zelig_out &lt;- zelig(Y ~ Z, model = \"oprobit\", data = data, cite = FALSE)\n    \n    sim_out &lt;- sim(zelig_out, x = setx(zelig_out, Z = 0), x1 = setx(zelig_out, Z = 1))\n    \n    zelig_df &lt;- zelig_qi_to_df(sim_out)\n    \n    predictions &lt;- with(zelig_df,\n                        (X1 + 2 * X2 + 3 * X3 + 4 * X4 + 5 * X5)[Z == 1] -\n                        (X1 + 2 * X2 + 3 * X3 + 4 * X4 + 5 * X5)[Z == 0])\n    \n    return_dat &lt;-\n      data.frame(term = \"Z\",\n                 estimate = mean(predictions),\n                 std.error = sd(predictions),\n                 conf.low = quantile(predictions, .025),\n                 conf.high = quantile(predictions, .975))\n    }\nWe can now declare the design:\nordered_probit_design &lt;- \n  \n  declare_model(\n    N = 200, \n    noise = rnorm(N),\n    potential_outcomes(Y ~ draw_ordered(1 * Z + noise, breaks = c(0, 0.75, 1, 1.25)))\n  ) + \n      \n  declare_assignment(Z = complete_ra(N)) +\n  \n  declare_inquiry(ate = mean(Y_Z_1 - Y_Z_0)) +\n  \n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  \n  declare_estimator(Y ~ Z, inquiry = \"ate\", label = \"DIM\") +\n  \n  declare_estimator(handler = label_estimator(ordered_probit), inquiry = \"ate\", label = \"Ordered Probit\")\nSample treatment/outcome data look like this:\nDiagnosis goes like this:\ndiagnose_design(ordered_probit_design)\nEstimator\nTerm\nN Sims\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\n\n\n\n\nDIM\nZ\n1000\n-0.00\n0.18\n1.00\n0.97\n1.35\n0.20\n0.20\n0.00\n1.35\n\n\n\n\n\n(0.01)\n(0.00)\n(0.00)\n(0.01)\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\nOrdered Probit\nZ\n1000\n-0.03\n0.18\nNA\n0.93\n1.33\n0.19\n0.18\nNA\n1.35\n\n\n\n\n\n(0.01)\n(0.00)\nNA\n(0.01)\n(0.01)\n(0.00)\n(0.00)\nNA\n(0.00)\nNext, we show a histogram of the difference-in-means and ordered probit estimates of the average treatment effect.\nStrikingly, difference-in-means and inferences using an ordered probit model perform very similarly.1 We often reach for ordered models to accomodate the fact that the outcome variable is ordinal and not cardinal, but if your goal is to estimate the average shift in outcomes in terms of shifts in points on the outcome variable you might be making more assumptions than you need.\nTo be clear, the point is not that there is no need to model the data properly. Modelling data generating processes lets you answer questions that difference-in-means does not. For example, assuming the model is correct, you get an estimate for treatment effects on a latent variable. Or you can ask how a treatment affects the probability of moving from a “3” to a “4” on this scale. So there may well be good reasons to use tailored models to model elaborate data generating processes, and buying the assumptions they require. But estimating average treatment effects doesn’t appear to be one of them."
  },
  {
    "objectID": "blog/posts/ordered-probit.html#footnotes",
    "href": "blog/posts/ordered-probit.html#footnotes",
    "title": "Estimating Average Treatment Effects with Ordered Probit: Is it worth it?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ordered probit estimates are very slightly biased. The bias disappears as the sample size increases.↩︎"
  },
  {
    "objectID": "blog/posts/process-tracing.html",
    "href": "blog/posts/process-tracing.html",
    "title": "What can you learn from simulating qualitative inference strategies?",
    "section": "",
    "text": "Qualitative process-tracing sometimes seeks to answer “cause of effects” claims using within-case data: how probable is the hypothesis that \\(X\\) did in fact cause \\(Y\\)? Fairfield and Charman (2017), for example, ask whether the right changed position on tax reform during the 2005 Chilean presidential election (\\(Y\\)) because of anti-inequality campaigns (\\(X\\)) by examining whether the case study narrative bears evidence that you would only expect to see if this were true.1 When inferential logics are so clearly articulated, it becomes possible to do design declaration and diagnosis. Here we declare a Bayesian process-tracing design and use it to think through choices about what kinds of within-case information have the greatest probative value.\nSay we want to evaluate a case-specific hypothesis, \\(H\\), regarding whether \\(Y\\) happened because \\(X\\) happened. The hypothesis is not that \\(X\\) is the only cause of \\(Y\\), but more simply whether \\(Y\\) would have been different had \\(X\\) been different. A researcher looks for “clues” or evidence, \\(E\\), in a case narrative or other qualitative data, which would be more or less surprising to see depending on whether \\(H\\) is true. Collier (2011) lays out the basic strategy. In a recent paper, Murtas, Dawid, and Musio (2017) show how to justify updating case level inferences from experimental data on moderators and mediators.\nFormally declaring and diagnosing such a procedure yields two non-obvious insights:"
  },
  {
    "objectID": "blog/posts/process-tracing.html#model-inquiry-data",
    "href": "blog/posts/process-tracing.html#model-inquiry-data",
    "title": "What can you learn from simulating qualitative inference strategies?",
    "section": "Model-Inquiry-Data",
    "text": "Model-Inquiry-Data\nIf we think of causal relations in counterfactual terms there are just four possible causal relationships between a binary \\(X\\) and a binary \\(Y\\):\n\nThe presence of natural resources could cause civil war (\\(X\\) causes \\(Y\\)).\nThe presence of natural resources could be the only thing preventing war (\\(\\neg X\\) causes \\(Y\\)).\nCivil war might happen irrespective of whether natural resources are present (\\(Y\\) irrespective of \\(X\\)).\nCivil war might not happen irrespective of whether natural resources are present (\\(\\neg Y\\) irrespective of \\(X\\)).\n\nFor the simulations, we will imagine we are in a world with 195 countries of which roughly 30% have natural resources (\\(X\\)) (that’s easy to specify). We will also specify a model in which civil war is governed by causal pathway 1 (\\(X\\) causes \\(Y\\)) in roughly 20% of countries, by pathway 2 (\\(\\neg X\\) causes \\(Y\\)) in only 10% of countries, by pathway 3 (\\(Y\\) irrespective of \\(X\\)) in 20% of countries, and by pathway 4 (\\(\\neg Y\\) irrespective of \\(X\\)) in half of all countries (that’s not so easy to specify and of course is information that is not available at the answer stage).\nIn addition, we imagine that there is further “process” data that is informative about causal relations. We imagine two types (see Collier (2011) for a discussion of clues of this type):\n\nA straw-in-the-wind clue. A straw-in-the-wind clue is an outcome that is somewhat more likely to be present if the hypothesized causal process is in operation and somewhat less likely if it is not. Let’s say, for example, that \\(E_1\\) is the national army taking control over natural resources during a civil war. We imagine that that’s likely to happen if the natural resources caused the war: \\(Pr(E_1 \\mid H) = .75\\). But even if the natural resources didn’t cause the war, the national army might still take over natural resources for other reasons, say \\(Pr(E_1 \\mid \\neg H) = .25\\).\nA smoking gun clue. A smoking gun clue is an outcome that is somewhat likely to be present if the stipulated hypothesis is true, but very unlikely if it is false. Say one of the antagonists was an armed group whose main name, aims, and ideology were centered around the capture and control of natural resources. This information provides a clue which might be really unlikely to arise in general, even if \\(H\\) is true. But it’s very informative if it is observed, since it’s so unlikely to arise if \\(H\\) is not true: it’s a “smoking gun.” Let’s say \\(Pr(E_2 \\mid H) = .3, Pr(E_2 \\mid \\neg H) = 0.05\\).\n\nThese clues might themselves be mediators, or moderators, or even arise post treatment, though we do not specify the full causal model that gives rise to them here. Rather, we simply define a step that generates these clue observations independently, conditional on the causal process. This is a strong assumption: the fact that an armed group formed in order to take resources (\\(E_2\\)) might convince the government to take over the natural resource (\\(E_1\\)) – or it might dissuade the government! We therefore relax this “Independent Clues” assumption below.\nThis gives us enough information to put down the stub of a design in which a model generates data with these features, an imaginary researcher samples one case from the \\(X=Y=1\\) group, and defines the question the researcher wants to ask about this case. Notice here the inquiry takes place after the sampling because we care about what happens in the specific case we chose.\n\ndesign_stub &lt;- \n    \n  declare_model(\n      N = 195, \n      X = rbinom(N, 1, .3) == 1,\n      causal_process = sample(c('X_causes_Y', 'X_causes_not_Y', 'Y_regardless', 'not_Y_regardless'), \n                              N, replace = TRUE, prob = c(.2, .1, .2, .5)),\n      Y = (X & causal_process == \"X_causes_Y\") |     \n          (!X & causal_process == \"X_causes_not_Y\") |\n          (causal_process == \"Y_regardless\"))  +\n  \n  declare_sampling(S = strata_rs(strata = (X == 1 & Y == 1), \n                                 strata_n = c(\"FALSE\" = 0, \"TRUE\" = 1))) +\n  \n  declare_measurement(\n    SIW_observed = rbinom(\n      n = N, size = 1, prob = ifelse(test = causal_process == 'X_causes_Y', .75, .25)),\n    SMG_observed = rbinom(\n      n = N, size = 1, prob = ifelse(test = causal_process == 'X_causes_Y', .3,  .05)),\n    label = \"Independent Clues\") +\n  \n  declare_inquiry(did_X_cause_Y = causal_process == 'X_causes_Y') \n\nSo far, a dataset from this design stub might look like this:\n\ndraw_data(design_stub) %&gt;% kable(digits = 2, align = \"c\")\n\n\n\n\nID\nX\ncausal_process\nY\nS\nSIW_observed\nSMG_observed\n\n\n\n\n055\nTRUE\nX_causes_Y\nTRUE\n1\n1\n0"
  },
  {
    "objectID": "blog/posts/process-tracing.html#answer-strategy",
    "href": "blog/posts/process-tracing.html#answer-strategy",
    "title": "What can you learn from simulating qualitative inference strategies?",
    "section": "Answer strategy",
    "text": "Answer strategy\nWe now turn to the answer strategy. For this, we’ll assume that at the analysis stage researchers use Bayes’ rule to figure out \\(Pr(H \\mid E)\\): the posterior probability that \\(X\\) caused \\(Y\\) in the case we chose, given the clue evidence we found. We make a function that calculates the posterior using Bayes’ rule:\n\\[Pr(H \\mid E) = \\frac{Pr(H) Pr(E|H)}{Pr(H)Pr(E\\mid H) + Pr(\\neg H)Pr(E\\mid\\neg H)}\\]\n\ncalculate_posterior &lt;- function(data, p_H, p_clue_found_H, p_clue_found_not_H, test, label) {\n  clue_found &lt;- data[, test]\n  p_E_H &lt;- ifelse(clue_found, p_clue_found_H, 1 - p_clue_found_H)\n  p_E_not_H &lt;- ifelse(clue_found, p_clue_found_not_H, 1 - p_clue_found_not_H)\n  data.frame(posterior_H = p_E_H * p_H / (p_E_H * p_H + p_E_not_H * (1 - p_H)), clue_found = clue_found)}\n\nBayes’ rule makes use of the probability of observing \\(E\\) if \\(H\\) is true and the probability of observing \\(E\\) if \\(H\\) is not true. The more different these probabilities are the more you learn from new data.\nWe also need to specify the imaginary researcher’s prior belief that \\(H\\) is true. The imaginary researcher knows that only two processes, 1 and 3 from above, could have generated the data \\(X = Y = 1\\). Thus, they might specify a “flat” prior: \\(Pr(H) = .5\\) (though they might have more informed beliefs from background knowledge).\nWe use the calculate_posterior() function we made above to declare two different answer strategies: one predicated on the straw-in-the-wind, and the other on the smoking gun.\n\ndesign &lt;-\n  \n  design_stub + \n  \n  declare_estimator(\n    test               = \"SIW_observed\", \n    p_H                = .5, \n    p_clue_found_H     = .75,\n    p_clue_found_not_H = .25,\n    label              = \"Straw in Wind\",\n    estimand           = \"did_X_cause_Y\",\n    handler            = label_estimator(calculate_posterior)) +\n  \n  declare_estimator(\n    test               = \"SMG_observed\", \n    p_H                = .5, \n    p_clue_found_H     = .30,\n    p_clue_found_not_H = .05,\n    label              = \"Smoking gun\",\n    estimand           = \"did_X_cause_Y\",\n    handler            = label_estimator(calculate_posterior))"
  },
  {
    "objectID": "blog/posts/process-tracing.html#footnotes",
    "href": "blog/posts/process-tracing.html#footnotes",
    "title": "What can you learn from simulating qualitative inference strategies?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example: “the former President [said] that the tax subsidy ‘never would have been eliminated if I had not taken [the opposition candidate] at his word’ when the latter publicly professed concern over inequality.”↩︎\nE.g. Collier (2011): of the four process-tracing tests, straws-in-the-wind are ``the weakest and place the least demand on the researcher’s knowledge and assumptions.’’ (826)↩︎"
  },
  {
    "objectID": "blog/posts/sometimes-blocking-can-reduce-your-precision.html",
    "href": "blog/posts/sometimes-blocking-can-reduce-your-precision.html",
    "title": "Sometimes blocking can reduce your precision",
    "section": "",
    "text": "You can often improve the precision of your randomized controlled trial with blocking: first gather similar units together into groups, then run experiments inside each little group, then average results across experiments. Block random assignment (sometimes called stratified random assignment) can be great—increasing precision with blocking is like getting extra sample size for free. Blocking works because it’s like controlling for a pre-treatment covariate in the “Data Strategy” rather than in the “Answer Strategy.” But sometimes it does more harm than good.\nThe standard experimental guidance is to block if you can in order to improve precision. (If for some reason you don’t have access to the pre-treatment covariates before the experiment is conducted, don’t fret, as the precision gains you would get from blocking on the front end can largely be made up by controlling on the back end.) In fact, even if you make blocks at random, you do as well as you would do under complete random assignment. For a textbook explanation of the benefits of blocking, see Gerber and Green (2012) (Section 3.6.1).\nBut is it possible to make things worse? Can you make a blocking that results in less precision than you would get from complete random assignment design?\nThe answer is yes. But you have to try really hard. If you organize units into groups that are internally unusually heterogeneous you can make things worse. For a formal analysis, see Imai (2008)."
  },
  {
    "objectID": "blog/posts/use-change-scores-or-control.html",
    "href": "blog/posts/use-change-scores-or-control.html",
    "title": "Use change scores or control for pre-treatment outcomes? Depends on the true data generating process",
    "section": "",
    "text": "We’re in an observational study setting in which treatment assignment was not controlled by the researcher. We have pre-treatment data on baseline outcomes and we’d like to incorporate them, mainly to decrease bias due to confounding and but also, ideally, to increase precision. One approach is to use the difference between pre and post outcomes as the outcome variable; another is to use the baseline data as a control. Which is better?\nFor a discussion see this post by Andrew Gelman and follow up comments by Winston Lin (that led us to the Allison reference we use below) and by Jens Hainmueller (that describes this logic).\nIn this post, design diagnosis shows that which approach is best depends on how outcomes are generated in the first place. A differencing approach might sometimes be effective at removing confounding bias but might do so at a cost of greater variance. Using this approach might not be optimal if confounding risks are small. Indeed, when there is no confounding (as in a randomized experiment), controlling is generally superior to differencing."
  },
  {
    "objectID": "blog/posts/use-change-scores-or-control.html#footnotes",
    "href": "blog/posts/use-change-scores-or-control.html#footnotes",
    "title": "Use change scores or control for pre-treatment outcomes? Depends on the true data generating process",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIndeed although the differencing approach has an “implied” coefficient of 1 on \\(Y_1\\), Allison (p 103) shows that the average estimated coefficient on \\(Y_1\\) from the control approach will be less than 1. In our case it is, on average, just 0.18.↩︎"
  },
  {
    "objectID": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html",
    "href": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html",
    "title": "You can’t speak meaningfully about spillovers without specifying an estimand",
    "section": "",
    "text": "A dangerous fact: it is quite possible to talk in a seemingly coherent way about strategies to answer a research question without ever properly specifying what the research question is. The risk is that you end up with the right solution to the wrong problem. The problem is particularly acute for studies where there are risks of “spillovers.”\nBy spillovers we mean situations where one unit’s outcome depends upon how another unit is assigned to treatment. You often hear worries that estimates of average treatment effects are biased in the presence of such spillovers. And in particular that when there are positive spillovers, estimates will be biased downwards. Sensible as these worries sound, try to state them formally and you can run into some difficulties. The key issue is that the assumption of no spillovers runs so deep that it is often invoked even prior to the definition of estimands. If you write the “average treatment effect” estimand using potential outcomes notation, as \\(E(Y(1) - Y(0))\\), you are already assuming that a unit’s outcomes depend only on its own assignment to treatment and not on how other units are assigned to treatment. The definition of the estimand leaves no space to even describe spillovers.\nIf there are in fact spillovers, then the estimand needs to be spelled out more carefully. In this case, the range of estimands to choose may be very wide and the appropriateness of different strategies is going to depend on which estimand you are shooting for.\nThis post shows how:\nAlong the way, we show how to modify designs by switching up steps at different points."
  },
  {
    "objectID": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#estimands-given-spillovers",
    "href": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#estimands-given-spillovers",
    "title": "You can’t speak meaningfully about spillovers without specifying an estimand",
    "section": "Estimands given spillovers",
    "text": "Estimands given spillovers\nThere are many different estimands that can take account of spillover effects, if there are any, but that correspond to the usual average treatment effects estimand when there are no spillovers. For instance, we can define the difference for a unit when it—and only it—is treated, compared to a situation in which no unit is treated. In potential outcomes notation that could be written, for unit 3, say, as:\n\\[\\tau_3 = Y(0,0,1,0,0,\\dots) -  Y(0,0,0,0,0,\\dots)\\]\nOne could then define a population estimand that is the average of all these differences over a population. Note that these differences specify different counterfactual assignment vectors for each individual. In the absence of spillovers, this estimand is equivalent to the usual average treatment effect. In the presence of spillovers, this estimand is well defined, whereas the usual average treatment effect estimand is not.\nThere are many other possibilities. For example, the difference in outcomes when no units are treated and all units are treated. Or the difference between not being treated when all others are, and being treated when all others are. Or, the difference a change in your own condition would make given that others are assigned to the value that they actually have (see e.g. Sävje, Aronow, and Hudgens (2017)). In fact, with \\(n\\) units and binary assignments, we can define \\(2^n\\times2^n\\) simple comparisons for each unit.\nBelow, we declare a design that allows for the possibility of spillovers. Diagnosing the design shows how severe the problem of spillovers can be for estimation. Modifying the design lets us explore different types of solutions."
  },
  {
    "objectID": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#how-spillovers-make-defining-and-estimating-average-effects-hard",
    "href": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#how-spillovers-make-defining-and-estimating-average-effects-hard",
    "title": "You can’t speak meaningfully about spillovers without specifying an estimand",
    "section": "How spillovers make defining and estimating average effects hard",
    "text": "How spillovers make defining and estimating average effects hard\nConsider a situation in which units are grouped into triplets, indexed \\(G\\). Suppose that there are 80 triplets. If any member of group \\(G\\) is treated, then all of its members receive some equal benefit (with marginal gains possibly increasing or decreasing in the numbers treated).\nWe can declare this design as follows:1\n\nN_groups &lt;- 80\nN_i_group &lt;- 3\nsd_i &lt;- 0.2\ngamma &lt;- 2\n\nmodel &lt;- declare_model(G = add_level(N = N_groups,\n                                               n = N_i_group),\n                                 i = add_level(N = n, I = 1:N, zeros = 0, ones = 1))\n\ndgp &lt;- function(i, Z, G, n) (sum(Z[G == G[i]])/n[i])^gamma + rnorm(1,0,sd_i)\n\ninquiry &lt;- declare_inquiry(Treat_one = mean(\n                 sapply(I, function(i) dgp(i, I==i, G, n) - dgp(i, zeros, G, n))\n                 ))\n\nassign    &lt;- declare_assignment(Z = complete_ra(N, prob = .5))\n\nmeasurement  &lt;- declare_measurement(Y = sapply(1:N, function(i) dgp(i, Z, G, n)))\n\nestimator &lt;- declare_estimator(Y ~ Z,  model = lm_robust,\n                               label = \"naive\", inquiry = \"Treat_one\")\n\nspillover_design &lt;- model + inquiry + assign + measurement + estimator\n\nThe most complex part of this design declaration involves the specification of the estimand. We define a helper function, dgp, which reports an individual’s outcome given the full treatment assignment vector. We then apply that function to each unit separately and take the average.\nThis design produces data that looks like this:\n\ndraw_data(spillover_design)\n\n\n\n\n\n\nG\nn\ni\nI\nzeros\nones\nZ\nY\n\n\n\n\n01\n3\n001\n1\n0\n1\n1\n1.1426481\n\n\n01\n3\n002\n2\n0\n1\n1\n0.8914236\n\n\n01\n3\n003\n3\n0\n1\n1\n1.1771557\n\n\n02\n3\n004\n4\n0\n1\n0\n0.0413922\n\n\n02\n3\n005\n5\n0\n1\n0\n-0.0904998\n\n\n02\n3\n006\n6\n0\n1\n1\n0.4877476\n\n\n\n\n\nAnd we can diagnose the design like this:\n\ndiagnosis &lt;- diagnose_design(spillover_design)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\n\n\n\n\n0.22\n0.23\n1.00\n0.00\n0.33\n0.05\n0.04\n0.00\n0.11\n\n\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\n\n\n\nWe see considerable bias here because the difference-in-means estimator does not account for within-group spillovers. Many units in the control condition are affected by the treatments received by other members of their group.\nInterestingly, in this case, we have an overestimate of the effect, even though there are positive spillovers. In the assumed model, there are increasing returns to spillovers. Units that are treated are also likely to be in groups with a higher proportion of treated units (since, by definition, at least one member of their group is already treated). On average, treated units thus receive more positive spillover than untreated units, leading to exaggerated estimates."
  },
  {
    "objectID": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#whether-a-sparser-sample-helps-depends-on-the-estimand-of-interest",
    "href": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#whether-a-sparser-sample-helps-depends-on-the-estimand-of-interest",
    "title": "You can’t speak meaningfully about spillovers without specifying an estimand",
    "section": "Whether a sparser sample helps depends on the estimand of interest",
    "text": "Whether a sparser sample helps depends on the estimand of interest\nThere are multiple solutions to this kind of problem. How well they work depends, however, on how well we understand the structure of spillover effects in the first place.\nOne approach is to alter assignment strategies (see, e.g., Bowers et al. (2018)). An even simpler one is to employ a sparser sample. This approach seems to go against one of the few near-universal principles of research design: study as many units as possible.\nBut here there may be some wisdom to it. Although more units generally means greater precision, there may be a cost to this when there are risks of spillovers. If a larger study means treating more units, and this means more units interfere with each other, you might end up with a spuriously precise, biased estimate.\nHere is an alternative design that implements the original data and analysis strategies on a sample of one subject per group:\n\nsparse_design &lt;- insert_step(spillover_design, after = \"Treat_one\",\n                          declare_sampling(S = strata_rs(strata = G, prob = 1/3)))\n\nNote that it is important that the sampling takes place here after the definition of the estimand. Spillovers operate as a function of population characteristics, not just sample characteristics.\nOur narrower sampling strategy reduces the N by two-thirds (from 240 to 80). But the results are now unbiased. Note that we did not change our original assignment strategy, in which units are assigned to treatment or to control with .5 probability. Rather, we changed the sampling strategy. Before, we kept whole groups and assigned no members or several members to treatment. We now select one member per group and assign them to treatment or to control. Because we only select one member per group, and spillovers only go within and not between groups, there is no way for our treatment or control units to receive spillovers.\nHere is a diagnosis of this sparser design:\n\ndiagnosis &lt;- diagnose_design(sparse_design, sims = sims)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\n\n\n\n\n0.00\n0.05\n0.70\n0.93\n0.11\n0.04\n0.04\n0.00\n0.11\n\n\n(0.00)\n(0.00)\n(0.01)\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\n\n\n\nWe have an unbiased design. Here though getting an unbiased design depended on having a good understanding of the nature of spillovers. In this case, we made use of the fact there would be no spillovers between groups.\nHowever, how good this solution is also depends on what exactly the estimand is. What if the estimand were the average difference in outcomes between a world in which no unit is treated and one in which all units are treated?\nWe answer this question by declaring a new design that adds in the new estimand and links it to the estimator.\nWe first define the steps:\n\nnew_estimand  &lt;- declare_inquiry(Treat_all = mean(\n                  sapply(I, function(i) dgp(i,ones,G,n) - dgp(i, zeros, G,n))\n                  ))\n\nnew_estimator &lt;- declare_estimator(Y ~ Z, model = lm_robust,\n                                   estimand = list(\"Treat_one\", \"Treat_all\"))\n\nWe then splice the steps in where we want them (estimand before the sampling, estimator after the sampling):\n\nsparse_design &lt;- insert_step(sparse_design, after = \"Treat_one\", new_estimand)\nsparse_design &lt;- replace_step(sparse_design, \"naive\", new_estimator)\n\nThe diagnosis highlights how poorly the sampling solution works for this new estimand:\n\ndiagnose_design(sparse_design)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry\nBias\nRMSE\nPower\nCoverage\nMean Estimate\nSD Estimate\nMean Se\nType S Rate\nMean Estimand\n\n\n\n\nTreat_all\n-0.89\n0.89\n0.68\n0.00\n0.11\n0.05\n0.04\n0.00\n1.00\n\n\n\n(0.00)\n(0.00)\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\nTreat_one\n0.00\n0.05\n0.68\n0.93\n0.11\n0.05\n0.04\n0.00\n0.11\n\n\n\n(0.00)\n(0.00)\n(0.01)\n(0.01)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n(0.00)\n\n\n\n\n\nThe sparser design fails because the treatment density is insufficient to reveal the potential outcomes required by the estimand: you cannot figure out the effects of saturation if you do not saturate.\nA future post will describe approaches to addressing spillovers that work through estimation strategies, rather than through sampling and assignment strategies.\n\nPost edited to reflect name change to spillover_designer() on 11/12/2018."
  },
  {
    "objectID": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#footnotes",
    "href": "blog/posts/you-cant-speak-meaningfully-about-spillovers-without-specifying-an-estimand.html#footnotes",
    "title": "You can’t speak meaningfully about spillovers without specifying an estimand",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, here we just made some minor edits to the code pulled from the DesignLibrary package using get_design_code(spillover_designer()).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "**Declare**Design",
    "section": "",
    "text": "Declare and diagnose your research design\n\n\nDeclareDesign is a set of software tools to plan, implement, analyze, and communicate about empirical research\n\n\n\n\nGet Started\n\n\nSoftware\n\n\nRead Book\n\n\n\n\n\n\n\n\nMIDA framework for describing research designs\n\n\n\n\n\nThe MIDA framework describes the four elements of any empirical research design:\n\nModel: the worlds you consider\nInquiry: the question you ask\nData strategy: sampling, treatment assignment, and measurement procedures\nAnswer strategy: estimation, testing, interpretation, and visualization procedures\n\nRead Chapter 5 of Research Design in the Social Sciences to learn how these four research design elements connect to reality and simulations that can be used to plan and improve research designs.\n\n\n\nDeclare-Diagnose-Redesign algorithm for designing research\n\nDeclare designs in code following the MIDA framework.\nDiagnose declared designs through Monte Carlo simulation to learn their properties, such as bias and power.\nRedesign data and answer strategy features to optimize designs under logistical, financial, and ethical constraints.\n\nHere is an illustration of using DeclareDesign for a two-arm randomized trial:\n\n1. Declare2. Diagnose3. Redesign\n\n\n\n\n\nlibrary(DeclareDesign)\n\nsample_size &lt;- 100\n\n# Declare a two-arm trial in code\ntwo_arm_trial &lt;-\n  declare_model(N = sample_size,\n                U = rnorm(N),\n                potential_outcomes(Y ~ 0.2 * Z + U)) +\n  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\n  declare_assignment(Z = complete_ra(N, prob = 0.5)) +\n  declare_measurement(Y = reveal_outcomes(Y ~ Z)) +\n  declare_estimator(Y ~ Z, inquiry = \"ATE\")\n\n\n\n\n# Draw a simulated dataset \ndraw_data(two_arm_trial)\n\n\n\n\n\n\nID\nU\nY_Z_0\nY_Z_1\nZ\nY\n\n\n\n\n001\n0.77\n0.77\n0.97\n0\n0.77\n\n\n002\n1.51\n1.51\n1.71\n0\n1.51\n\n\n003\n-0.73\n-0.73\n-0.53\n1\n-0.53\n\n\n\n\n\n\n# Obtain a simulated estimate and estimand\nrun_design(two_arm_trial)\n\n\n\n\n\n\nestimate\nstd.error\np.value\ninquiry\nestimand\n\n\n\n\n0.33\n0.2\n0.1\nATE\n0.2\n\n\n\n\n\n\n\n\n\n\n\n\n# Simulate the research design 500 times and \n#   summarize the simulations\ndiagnosis &lt;- diagnose_design(two_arm_trial, sims = 500)\ntidy(diagnosis)\n\n\n\n\n\n\ndiagnosand\nestimate\nstd.error\n\n\n\n\nmean_estimand\n0.200\n0.000\n\n\nmean_estimate\n0.206\n0.009\n\n\nbias\n0.006\n0.009\n\n\nsd_estimate\n0.202\n0.006\n\n\nrmse\n0.201\n0.006\n\n\npower\n0.178\n0.019\n\n\ncoverage\n0.942\n0.011\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# Visualize simulated sampling distribution\nggplot(data = get_simulations(diagnosis), \n       aes(x = estimate)) +\n  geom_histogram() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Redesign over sample size and calculate power\ndiagnosis &lt;-\n  two_arm_trial |&gt;\n  redesign(sample_size = c(250, 500, 750, 1000, 1250)) |&gt;\n  diagnose_designs() |&gt;\n  tidy() |&gt;\n  filter(diagnosand == \"power\")\n\n# Visualize power curve over sample sizes\nggplot(diagnosis, aes(sample_size, estimate)) + \n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibrary of common research designs\nThe MIDA framework can accomodate observational and experimental, descriptive and causal, qualitative and quantitative research designs. Part III of Research Design in the Social Sciences illustrates the framework for these common designs.\n\n\nObservational designs for descriptive inference\nSimple random sampling Cluster random sampling Multi-level regression and poststratification Index creation\n\n\nObservational designs for causal inference\nProcess tracing Selection-on-observables Difference-in-differences Instrumental variables Regression discontinuity designs\n\n\nExperimental designs for descriptive inference\nAudit experiments List experiments Conjoint experiments Behavioral games\n\n\nExperimental designs for causal inference\nTwo-arm randomized experiments Block-randomized experiments Cluster-randomized experiments Subgroup designs Factorial experiments Encouragement designs Placebo-controlled experiments Stepped-wedge experiments Randomized saturation experiments Experiments over networks\n\n\nComplex designs\nDiscovery using causal forests Structural estimation Meta-analysis Multi-site studies\n\n\n\n\n\n\nGet Started\n\n\nSoftware\n\n\nRead Book\n\n\nRead Blog\n\n\nAbout"
  }
]